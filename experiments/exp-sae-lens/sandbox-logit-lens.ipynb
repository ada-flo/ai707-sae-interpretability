{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb37914f",
   "metadata": {},
   "source": [
    "# Understanding SAE Features with the Logit Lens\n",
    "\n",
    "This notebook demonstrates how to use the mats_sae_training library to perform the analysis documented the post \"[Understanding SAE Features with the Logit Lens](https://www.alignmentforum.org/posts/qykrYY6rXXM7EEs8Q/understanding-sae-features-with-the-logit-lens)\".\n",
    "\n",
    "As such, the notebook will include sections for:\n",
    "\n",
    "- Loading in GPT2-Small Residual Stream SAEs from Huggingface.\n",
    "- Performing Virtual Weight Based Analysis of features (specifically looking at the logit weight distributions).\n",
    "- Programmatically opening neuronpedia tabs to engage with public dashboards on [neuronpedia](https://www.neuronpedia.org/).\n",
    "- Performing Token Set Enrichment Analysis (based on Gene Set Enrichment Analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b9c19",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "Here we'll load various functions for things like:\n",
    "\n",
    "- downloading and loading our SAEs from huggingface.\n",
    "- opening neuronpedia from a jupyter cell.\n",
    "- calculating statistics of the logit weight distributions.\n",
    "- performing Token Set Enrichment Analysis (TSEA) and plotting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91195eb",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac45556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature statistics\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from sae_lens.saes.sae import SAE\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_feature_property_df(sae: SAE, feature_sparsity: torch.Tensor):\n",
    "    \"\"\"\n",
    "    feature_property_df = get_feature_property_df(sae, log_feature_density.cpu())\n",
    "    \"\"\"\n",
    "\n",
    "    W_dec_normalized = (\n",
    "        sae.W_dec.cpu()\n",
    "    )  # / sparse_autoencoder.W_dec.cpu().norm(dim=-1, keepdim=True)\n",
    "    W_enc_normalized = (sae.W_enc.cpu() / sae.W_enc.cpu().norm(dim=-1, keepdim=True)).T\n",
    "\n",
    "    d_e_projection = (W_dec_normalized * W_enc_normalized).sum(-1)\n",
    "    b_dec_projection = sae.b_dec.cpu() @ W_dec_normalized.T\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"log_feature_sparsity\": feature_sparsity + 1e-10,\n",
    "            \"d_e_projection\": d_e_projection,\n",
    "            # \"d_e_projection_normalized\": d_e_projection_normalized,\n",
    "            \"b_enc\": sae.b_enc.detach().cpu(),\n",
    "            \"b_dec_projection\": b_dec_projection,\n",
    "            \"feature\": list(range(sae.cfg.d_sae)),  # type: ignore\n",
    "            \"dead_neuron\": (feature_sparsity < -9).cpu(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_stats_df(projection: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with the mean, std, skewness and kurtosis of the projection\n",
    "    \"\"\"\n",
    "    mean = projection.mean(dim=1, keepdim=True)\n",
    "    diffs = projection - mean\n",
    "    var = (diffs**2).mean(dim=1, keepdim=True)\n",
    "    std = torch.pow(var, 0.5)\n",
    "    zscores = diffs / std\n",
    "    skews = torch.mean(torch.pow(zscores, 3.0), dim=1)\n",
    "    kurtosis = torch.mean(torch.pow(zscores, 4.0), dim=1)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": range(len(skews)),\n",
    "            \"mean\": mean.numpy().squeeze(),\n",
    "            \"std\": std.numpy().squeeze(),\n",
    "            \"skewness\": skews.numpy(),\n",
    "            \"kurtosis\": kurtosis.numpy(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_all_stats_dfs(\n",
    "    gpt2_small_sparse_autoencoders: dict[str, SAE],  # [hook_point, sae]\n",
    "    gpt2_small_sae_sparsities: dict[str, torch.Tensor],  # [hook_point, sae]\n",
    "    model: HookedTransformer,\n",
    "    cosine_sim: bool = False,\n",
    "):\n",
    "    stats_dfs = []\n",
    "    pbar = tqdm(gpt2_small_sparse_autoencoders.keys())\n",
    "    for key in pbar:\n",
    "        layer = int(key.split(\".\")[1])\n",
    "        sparse_autoencoder = gpt2_small_sparse_autoencoders[key]\n",
    "        pbar.set_description(f\"Processing layer {sparse_autoencoder.cfg.hook_name}\")\n",
    "        W_U_stats_df_dec, _ = get_W_U_W_dec_stats_df(\n",
    "            sparse_autoencoder.W_dec.cpu(), model, cosine_sim\n",
    "        )\n",
    "        log_feature_sparsity = gpt2_small_sae_sparsities[key].detach().cpu()\n",
    "        W_U_stats_df_dec[\"log_feature_sparsity\"] = log_feature_sparsity\n",
    "        W_U_stats_df_dec[\"layer\"] = layer + (1 if \"post\" in key else 0)\n",
    "        stats_dfs.append(W_U_stats_df_dec)\n",
    "\n",
    "    return pd.concat(stats_dfs, axis=0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_W_U_W_dec_stats_df(\n",
    "    W_dec: torch.Tensor, model: HookedTransformer, cosine_sim: bool = False\n",
    ") -> tuple[pd.DataFrame, torch.Tensor]:\n",
    "    W_U = model.W_U.detach().cpu()\n",
    "    if cosine_sim:\n",
    "        W_U = W_U / W_U.norm(dim=0, keepdim=True)\n",
    "    dec_projection_onto_W_U = W_dec @ W_U\n",
    "    W_U_stats_df = get_stats_df(dec_projection_onto_W_U)\n",
    "    return W_U_stats_df, dec_projection_onto_W_U"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
