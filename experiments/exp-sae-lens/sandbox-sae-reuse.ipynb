{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6fc430",
   "metadata": {},
   "source": [
    "# Reusing the TinyStories SAE\n",
    "\n",
    "This notebook loads the SAE trained in `tutorial-training-sae.ipynb` (saved under `output/`) and shows how to attach it to the model, peek at sparsity stats, and run a quick comparison with/without SAE reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf95d3",
   "metadata": {},
   "source": [
    "## How to read this notebook\n",
    "- Load the TinyStories SAE and show where it hooks (`hook_name`, dims, dtype/device).\n",
    "- Check saved sparsity and a quick prompt loss comparison with/without reconstruction.\n",
    "- Inspect which SAE features fire on a prompt and how a few features tilt the logits.\n",
    "- Use the reconstruction metrics cell to spot if a new checkpoint looks off (high MSE or low explained variance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d348b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Basic imports and a simple device picker.\n",
    "# Feel free to switch to CPU if you don't have a GPU handy.\n",
    "from pathlib import Path\n",
    "import contextlib\n",
    "import re\n",
    "import torch\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# Choose a device automatically so the rest of the notebook just works.\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else \n",
    "    \"mps\" if torch.backends.mps.is_available() else \n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "sae_dir = Path(\"output\")\n",
    "assert sae_dir.exists(), \"The SAE folder from training should live in ./output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a9ea948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-1L-21M into HookedTransformer\n",
      "SAE attaches to hook: blocks.0.hook_mlp_out\n",
      "Latent dims: 16384\n",
      "SAE dtype/device: torch.float32 on cuda\n"
     ]
    }
   ],
   "source": [
    "# Load the TinyStories model and the trained SAE from disk.\n",
    "# HookedSAETransformer is a drop-in replacement for HookedTransformer that knows how to host SAEs.\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained(\"tiny-stories-1L-21M\", device=device)\n",
    "model.eval();\n",
    "\n",
    "sae = SAE.load_from_disk(sae_dir, device=device)\n",
    "sae.eval();\n",
    "\n",
    "def tl_hook_name_from_hf(path: str) -> str:\n",
    "    m = re.fullmatch(r\"model\\.layers\\.(\\d+)\\.mlp\\.down_proj\", path)\n",
    "    if m:\n",
    "        return f\"blocks.{m.group(1)}.hook_mlp_out\"\n",
    "    m = re.fullmatch(r\"model\\.layers\\.(\\d+)\\.self_attn\\.o_proj\", path)\n",
    "    if m:\n",
    "        return f\"blocks.{m.group(1)}.attn.hook_result\"\n",
    "    return path\n",
    "\n",
    "sae.cfg.metadata.hook_name = tl_hook_name_from_hf(sae.cfg.metadata.hook_name)\n",
    "\n",
    "print(f\"SAE attaches to hook: {sae.cfg.metadata.hook_name}\")\n",
    "print(f\"Latent dims: {sae.cfg.d_sae}\")\n",
    "print(f\"SAE dtype/device: {sae.dtype} on {sae.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe4db9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated active features per token (L0): 1256.2 out of 16384\n"
     ]
    }
   ],
   "source": [
    "# Quick look at sparsity statistics that were saved after training.\n",
    "# The tensor stores log sparsity per feature; exp(log_sparsity) is the probability a feature is active on a token.\n",
    "log_sparsity = load_file(sae_dir / \"sparsity.safetensors\")[\"sparsity\"]\n",
    "est_l0 = torch.exp(log_sparsity).mean().item() * log_sparsity.numel()\n",
    "print(f\"Estimated active features per token (L0): {est_l0:.1f} out of {log_sparsity.numel()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2e14d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper utilities so we can flip the SAE on/off easily.\n",
    "# The context manager model.saes(...) temporarily patches the model to route activations through the SAE.\n",
    "def generate_text(prompt: str, with_sae: bool = False, **gen_kwargs) -> str:\n",
    "    ctx = model.saes(saes=[sae]) if with_sae else contextlib.nullcontext()\n",
    "    with torch.no_grad(), ctx:\n",
    "        return model.generate(\n",
    "            prompt,\n",
    "            max_new_tokens=80,\n",
    "            temperature=0.7,\n",
    "            stop_at_eos=True,\n",
    "            verbose=False,\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "def loss_on_prompt(prompt: str, with_sae: bool = False) -> float:\n",
    "    ctx = model.saes(saes=[sae]) if with_sae else contextlib.nullcontext()\n",
    "    with torch.no_grad(), ctx:\n",
    "        tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "        logits = model(tokens)\n",
    "        # Compare predicted next tokens to the ground truth tokens.\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            logits[0, :-1].reshape(-1, logits.size(-1)),\n",
    "            tokens[0, 1:].reshape(-1),\n",
    "        )\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ffe9925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Base model ---\n",
      "Once upon a time, a curious robot learned to behave. He thanked the tank and drove away, but he was still very careful. He never forgot the lesson he learned that day that being careless can be dangerous.\n",
      "\n",
      "--- With SAE reconstruction ---\n",
      "Once upon a time, a curious robot learned to be careful and not touch it.\n",
      "\n",
      "One day, a little girl named Lucy heard a loud noise. She was scared and wanted to go to the park to play. She ran to the swings and grabbed the handle. She pulled it out of the box, and the buttons were very happy.\n",
      "\n",
      "She put the new toy in the box and went to the park. She had a lot\n",
      "Next-token loss without SAE: 3.889\n",
      "Next-token loss with SAE:    3.731\n"
     ]
    }
   ],
   "source": [
    "# Compare generations with and without SAE reconstruction.\n",
    "prompt = \"Once upon a time, a curious robot learned to\"\n",
    "\n",
    "base_story = generate_text(prompt, with_sae=False)\n",
    "sae_story = generate_text(prompt, with_sae=True)\n",
    "\n",
    "print(\"--- Base model ---\")\n",
    "print(base_story)\n",
    "print(\"--- With SAE reconstruction ---\")\n",
    "print(sae_story)\n",
    "\n",
    "base_loss = loss_on_prompt(prompt, with_sae=False)\n",
    "sae_loss = loss_on_prompt(prompt, with_sae=True)\n",
    "print(f\"Next-token loss without SAE: {base_loss:.3f}\")\n",
    "print(f\"Next-token loss with SAE:    {sae_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daf60479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top activated features on the last token:\n",
      "  Feature  5242 -> activation 5.26\n",
      "  Feature  1195 -> activation 1.94\n",
      "  Feature 16084 -> activation 1.73\n",
      "  Feature  5574 -> activation 1.65\n",
      "  Feature 14552 -> activation 1.53\n"
     ]
    }
   ],
   "source": [
    "# Peek at which SAE features fire on the last token of the prompt.\n",
    "# We grab the MLP activations, encode them with the SAE, and show the top activations.\n",
    "with torch.no_grad():\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "    _, cache = model.run_with_cache(tokens, names_filter=[sae.cfg.metadata.hook_name])\n",
    "    mlp_out = cache[sae.cfg.metadata.hook_name]\n",
    "    feature_acts = sae.encode(mlp_out)\n",
    "\n",
    "last_token_acts = feature_acts[0, -1]\n",
    "values, indices = torch.topk(last_token_acts, k=5)\n",
    "print(\"Top activated features on the last token:\")\n",
    "for val, idx in zip(values.tolist(), indices.tolist()):\n",
    "    print(f\"  Feature {idx:5d} -> activation {val:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b7b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction quality on the prompt: how close is SAE decode to the raw MLP activations?\n",
    "with torch.no_grad():\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "    _, cache = model.run_with_cache(tokens, names_filter=[sae.cfg.metadata.hook_name])\n",
    "    mlp_out = cache[sae.cfg.metadata.hook_name]\n",
    "    feature_acts = sae.encode(mlp_out)\n",
    "    recon = sae.decode(feature_acts)\n",
    "\n",
    "    mse = torch.nn.functional.mse_loss(recon, mlp_out).item()\n",
    "    l2_orig = mlp_out.pow(2).mean().sqrt().item()\n",
    "    l2_err = (recon - mlp_out).pow(2).mean().sqrt().item()\n",
    "    explained = 1 - (l2_err**2 / (l2_orig**2 + 1e-9))\n",
    "    density = (feature_acts.abs() > 1e-6).float().mean().item()\n",
    "\n",
    "print(\n",
    "    f\"Recon MSE: {mse:.6f}\n",
    "\"\n",
    "    f\"Orig L2 (mean): {l2_orig:.4f}\n",
    "\"\n",
    "    f\"Error L2 (mean): {l2_err:.4f}\n",
    "\"\n",
    "    f\"Explained variance (approx): {explained:.4f}\n",
    "\"\n",
    "    f\"Mean activation density: {density:.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817cbb04",
   "metadata": {},
   "source": [
    "### Reading the metrics\n",
    "- If `Recon MSE` or `Error L2` spikes compared to past runs, the SAE may be mis-specified or checkpoint is wrong.\n",
    "- `Explained variance` near 1.0 means the reconstruction matches MLP activations closely; values near 0 indicate poor fit.\n",
    "- `Mean activation density` captures sparsity; large jumps imply the L1 coefficient or normalization changed.\n",
    "- Compare the base vs SAE next-token losses to see whether reconstruction is neutral or harmful for the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b96af27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random feature IDs: [85, 667, 1304, 1564, 6450, 7750, 9467, 10254, 11155, 15182]\n",
      "Base model last-token distribution:\n",
      "   be          0.147\n",
      "   explore     0.101\n",
      "   fly         0.083\n",
      "   play        0.050\n",
      "   work        0.022\n",
      "   try         0.019\n",
      "   spin        0.019\n",
      "   use         0.017\n",
      "   make        0.016\n",
      "   do          0.016\n",
      "\n",
      "Only these 10 features reconstructed:\n",
      "   to          0.737\n",
      "  .            0.096\n",
      "   and         0.094\n",
      "   the         0.041\n",
      "  ,            0.014\n",
      "   in          0.008\n",
      "   be          0.002\n",
      "   with        0.001\n",
      "   a           0.001\n",
      "   that        0.001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample 10 random SAE features and see the logits they produce on the prompt.\n",
    "# We keep only those features active (zeroing the rest), then swap that reconstruction into the model.\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)  # make the random feature pick reproducible\n",
    "num_features = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "\n",
    "    # Baseline logits for comparison\n",
    "    base_logits = model(tokens)\n",
    "\n",
    "    # Cache the MLP outputs, encode with the SAE, and pick 10 random features\n",
    "    _, cache = model.run_with_cache(tokens, names_filter=[sae.cfg.metadata.hook_name])\n",
    "    mlp_out = cache[sae.cfg.metadata.hook_name]\n",
    "    feature_acts = sae.encode(mlp_out)\n",
    "\n",
    "    rand_idx = torch.randperm(feature_acts.size(-1), device=device)[:num_features]\n",
    "    kept_features = torch.zeros_like(feature_acts)\n",
    "    kept_features[..., rand_idx] = feature_acts[..., rand_idx]\n",
    "\n",
    "    # Decode just these features back into the MLP space\n",
    "    recon_from_subset = sae.decode(kept_features)\n",
    "\n",
    "    # Swap the original MLP output with the reconstruction from those 10 features\n",
    "    def swap_mlp_out(acts, hook):\n",
    "        return recon_from_subset\n",
    "\n",
    "    hooked_logits = model.run_with_hooks(\n",
    "        tokens, fwd_hooks=[(sae.cfg.metadata.hook_name, swap_mlp_out)]\n",
    "    )\n",
    "\n",
    "\n",
    "def show_top(logits, label, k=10):\n",
    "    probs = logits.softmax(-1)\n",
    "    vals, idxs = torch.topk(probs, k)\n",
    "    print(label)\n",
    "    for v, i in zip(vals.tolist(), idxs.tolist()):\n",
    "        # tokenizer.decode gives a nice string for the token id\n",
    "        print(f\"  {model.tokenizer.decode([i]):12s} {v:.3f}\")\n",
    "    print()\n",
    "\n",
    "base_last = base_logits[0, -1]\n",
    "hooked_last = hooked_logits[0, -1]\n",
    "print(f\"Random feature IDs: {sorted(rand_idx.tolist())}\")\n",
    "show_top(base_last, \"Base model last-token distribution:\")\n",
    "show_top(hooked_last, \"Only these 10 features reconstructed:\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp-sae-lens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}