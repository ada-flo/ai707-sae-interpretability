{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e146048",
   "metadata": {},
   "source": [
    "# SAE + Llama3 8B minimal test\n",
    "Load latest SAE, load model directly via HookedSAETransformer.from_pretrained (no HF wrapper), and generate with the SAE attached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa37efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd/jdh/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from sae_lens import SAE, HookedSAETransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfcb9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SAE from runs/20251213_002417_llama3_8b/final_sae, hook=model.layers.15.mlp.down_proj, d_in=4096, d_sae=16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd/jdh/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/sae_lens/saes/sae.py:248: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load latest SAE\n",
    "runs_root = Path('runs')\n",
    "candidate_runs = sorted([p for p in runs_root.glob('*_llama3_8b') if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not candidate_runs:\n",
    "    raise FileNotFoundError('No *_llama3_8b run directories found.')\n",
    "run_dir = candidate_runs[0]\n",
    "sae_dir = run_dir / 'final_sae'\n",
    "if not sae_dir.exists():\n",
    "    sae_dir = run_dir\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DTYPE = torch.bfloat16 if DEVICE == 'cuda' else torch.float32\n",
    "sae = SAE.load_from_disk(sae_dir, device=DEVICE)\n",
    "sae.eval()\n",
    "\n",
    "# Translate HF module path to TransformerLens hook name\n",
    "import re\n",
    "def tl_hook_name(path: str) -> str:\n",
    "    m = re.fullmatch(r'model\\.layers\\.(\\d+)\\.mlp\\.down_proj', path)\n",
    "    if m:\n",
    "        return f'blocks.{m.group(1)}.hook_mlp_out'\n",
    "    m = re.fullmatch(r'model\\.layers\\.(\\d+)\\.self_attn\\.o_proj', path)\n",
    "    if m:\n",
    "        return f'blocks.{m.group(1)}.attn.hook_result'\n",
    "    return path\n",
    "\n",
    "sae.cfg.metadata.hook_name = tl_hook_name(sae.cfg.metadata.hook_name)\n",
    "print(f\"Loaded SAE from {sae_dir}, hook={sae.cfg.metadata.hook_name}, d_in={sae.cfg.d_in}, d_sae={sae.cfg.d_sae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0ddafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 85.60it/s]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B into HookedTransformer\n",
      "Model ready on cuda with dtype torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Load model directly via TransformerLens\n",
    "model_name = sae.cfg.metadata.model_name\n",
    "model = HookedSAETransformer.from_pretrained(\n",
    "    model_name,\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    "    move_to_device=True,\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model ready on {DEVICE} with dtype {DTYPE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b264bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Hook model.layers.15.mlp.down_proj not in model.hook_dict; sample keys: ['hook_embed', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized'] ...\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m sae\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mhook_name\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mhook_dict:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHook \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in model.hook_dict; sample keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mhook_dict\u001b[38;5;241m.\u001b[39mkeys())[:\u001b[38;5;241m5\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto_tokens(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHello world\u001b[39m\u001b[38;5;124m'\u001b[39m, prepend_bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Hook model.layers.15.mlp.down_proj not in model.hook_dict; sample keys: ['hook_embed', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized'] ...\""
     ]
    }
   ],
   "source": [
    "# Quick activation sanity check\n",
    "hook_name = sae.cfg.metadata.hook_name\n",
    "if hook_name not in model.hook_dict:\n",
    "    raise KeyError(f\"Hook {hook_name} not in model.hook_dict; sample keys: {list(model.hook_dict.keys())[:5]} ...\")\n",
    "with torch.no_grad():\n",
    "    tokens = model.to_tokens('Hello world', prepend_bos=True)\n",
    "    _, cache = model.run_with_cache(tokens, names_filter=[hook_name])\n",
    "    acts = cache[hook_name]\n",
    "    print('Hook activation shape', acts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with SAE attached\n",
    "prompt = \"Once upon a time, a helpful robot\"\n",
    "with model.saes(saes=[sae]):\n",
    "    output = model.generate(\n",
    "        prompt,\n",
    "        max_new_tokens=60,\n",
    "        temperature=0.7,\n",
    "        stop_at_eos=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp-sae-lens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}