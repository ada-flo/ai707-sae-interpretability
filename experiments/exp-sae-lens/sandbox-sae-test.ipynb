{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e146048",
   "metadata": {},
   "source": [
    "# SAE + Llama3 8B minimal test\n",
    "Load latest SAE, load model directly via HookedSAETransformer.from_pretrained (no HF wrapper), and generate with the SAE attached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de4ae86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NVIDIA RTX A6000\n",
      "1 NVIDIA RTX A6000\n",
      "2 NVIDIA RTX A6000\n",
      "3 NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bc19c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3, dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "GPU_INDEX = 3\n",
    "\n",
    "DEVICE = (\n",
    "    f\"cuda:{GPU_INDEX}\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "DTYPE = torch.bfloat16 if DEVICE.startswith('cuda') else torch.float32\n",
    "print(f\"Using device: {DEVICE}, dtype: {DTYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfa37efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "from sae_lens import SAE, HookedSAETransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cfcb9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SAE from runs/20251213_174540_llama3_8b/final_sae, hook=blocks.15.hook_resid_pre, d_in=4096, d_sae=16384\n"
     ]
    }
   ],
   "source": [
    "# Load latest SAE\n",
    "runs_root = Path('runs')\n",
    "candidate_runs = sorted([p for p in runs_root.glob('*_llama3_8b') if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not candidate_runs:\n",
    "    raise FileNotFoundError('No *_llama3_8b run directories found.')\n",
    "run_dir = candidate_runs[0]\n",
    "sae_dir = run_dir / 'final_sae'\n",
    "if not sae_dir.exists():\n",
    "    sae_dir = run_dir\n",
    "\n",
    "sae = SAE.load_from_disk(sae_dir, device=DEVICE)\n",
    "sae.eval()\n",
    "\n",
    "# Translate HF module path to TransformerLens hook name\n",
    "import re\n",
    "def tl_hook_name(path: str) -> str:\n",
    "    m = re.fullmatch(r'model\\.layers\\.(\\d+)\\.mlp\\.down_proj', path)\n",
    "    if m:\n",
    "        return f'blocks.{m.group(1)}.hook_mlp_out'\n",
    "    m = re.fullmatch(r'model\\.layers\\.(\\d+)\\.self_attn\\.o_proj', path)\n",
    "    if m:\n",
    "        return f'blocks.{m.group(1)}.attn.hook_result'\n",
    "    return path\n",
    "\n",
    "sae.cfg.metadata.hook_name = tl_hook_name(sae.cfg.metadata.hook_name)\n",
    "print(f\"Loaded SAE from {sae_dir}, hook={sae.cfg.metadata.hook_name}, d_in={sae.cfg.d_in}, d_sae={sae.cfg.d_sae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de0ddafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b588d85a36254aeda87340b06e487646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B into HookedTransformer\n",
      "Model ready on cuda:3 with dtype torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Load model directly via TransformerLens\n",
    "model_name = sae.cfg.metadata.model_name\n",
    "model = HookedSAETransformer.from_pretrained(\n",
    "    model_name,\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    "    move_to_device=True,\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model ready on {DEVICE} with dtype {DTYPE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20b264bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook activation shape torch.Size([1, 3, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Quick activation sanity check\n",
    "hook_name = sae.cfg.metadata.hook_name\n",
    "if hook_name not in model.hook_dict:\n",
    "    raise KeyError(f\"Hook {hook_name} not in model.hook_dict; sample keys: {list(model.hook_dict.keys())[:5]} ...\")\n",
    "with torch.no_grad():\n",
    "    tokens = model.to_tokens('Hello world', prepend_bos=True)\n",
    "    _, cache = model.run_with_cache(tokens, names_filter=[hook_name])\n",
    "    acts = cache[hook_name]\n",
    "    print('Hook activation shape', acts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, a helpful robot named Nelly, a dog named Pip, and an unnamed baby were left behind on the surface of Mars.\n",
      "The Moon is 4.5 billion years old, and its remaining magnetism and various other geologic features are clues to its origins.\n"
     ]
    }
   ],
   "source": [
    "# Generate with SAE attached\n",
    "prompt = \"Once upon a time, a helpful robot\"\n",
    "with model.saes(saes=[sae]):\n",
    "    output = model.generate(\n",
    "        prompt,\n",
    "        max_new_tokens=60,\n",
    "        temperature=0.7,\n",
    "        stop_at_eos=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
