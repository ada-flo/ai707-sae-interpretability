{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "967f425e",
   "metadata": {},
   "source": [
    "# Train a Sparse Autoencoder on Llama 3 8B (single A6000)\n",
    "\n",
    "This notebook mirrors the SAELens training flow but targets `meta-llama/Meta-Llama-3-8B` on a single 24GB 4090. The defaults aim to be friendly to limited VRAM: mid-layer MLP activations, moderate context length, and a mid-size SAE. Comments are written for ML beginners, with notes on how to downscale if you see OOMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c2a113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NVIDIA RTX A6000\n",
      "1 NVIDIA RTX A6000\n",
      "2 NVIDIA RTX A6000\n",
      "3 NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121d444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "GPU_INDEX = 3\n",
    "\n",
    "device = (\n",
    "    f\"cuda:{GPU_INDEX}\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9062d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level knobs. Defaults shrunk to ease VRAM on a 24GB 4090.\n",
    "# If you hit OOM, lower d_sae first, then train_batch_size_tokens, then context_size.\n",
    "hook_layer = 15  # middle layer of Llama 3 8B (32 layers total)\n",
    "# TransformerLens hook naming (residual before block L updates)\n",
    "hook_name = f\"blocks.{hook_layer}.hook_resid_pre\"\n",
    "\n",
    "# Model dims for Llama 3 8B\n",
    "hidden_size = 4096  # d_model\n",
    "mlp_width = 14336   # intermediate size\n",
    "\n",
    "# SAE size: d_sae controls feature count.\n",
    "d_in = hidden_size  # residual stream width\n",
    "\n",
    "d_sae = 16384  # safer default; try 32768 if VRAM allows, 65536 if roomy\n",
    "\n",
    "# Training shape controls memory directly.\n",
    "context_size = 256          # shorter contexts are cheaper; try 512 if you have headroom\n",
    "train_batch_size_tokens = 1024  # tokens per step; reduce to 512 if still OOM, raise to 2048 if comfy\n",
    "total_training_tokens = 10_000_000  # scale down for quicker smoke test (e.g., 2-3M)\n",
    "\n",
    "l1_coeff = 2.5  # sparsity strength; try 1.0 (denser) or 5.0 (sparser) to explore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6edd3",
   "metadata": {},
   "source": [
    "### Hook placement and naming\n",
    "- TransformerLens (HookedTransformer): residual hooks (`hook_resid_pre/post`), MLP (`hook_mlp_out`), attention (`attn.hook_result`).\n",
    "- Pick the hook to match your target signal and set `d_in` to that hook's width (4096 for Llama3 8B residual).\n",
    "This notebook trains against the TL residual hook (`blocks.<L>.hook_resid_pre`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d98baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd1/daeheon/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LanguageModelSAERunnerConfig(sae=StandardTrainingSAEConfig(d_in=4096, d_sae=16384, dtype='float32', device='cpu', apply_b_dec_to_input=False, normalize_activations='expected_average_only_in', reshape_activations='none', metadata=SAEMetadata({'sae_lens_version': '6.22.3', 'sae_lens_training_version': '6.22.3'}), decoder_init_norm=0.1, l1_coefficient=2.5, lp_norm=1.0, l1_warm_up_steps=500), model_name='meta-llama/Meta-Llama-3-8B', model_class_name='HookedTransformer', hook_name='blocks.15.hook_resid_pre', hook_eval='NOT_IN_USE', hook_head_index=None, dataset_path='monology/pile-uncopyrighted', dataset_trust_remote_code=True, streaming=True, is_dataset_tokenized=False, context_size=256, use_cached_activations=False, cached_activations_path=None, from_pretrained_path=None, n_batches_in_buffer=16, training_tokens=10000000, store_batch_size_prompts=32, seqpos_slice=(None,), disable_concat_sequences=False, sequence_separator_token='bos', device='cuda:3', act_store_device='cuda:3', seed=42, dtype='bfloat16', prepend_bos=True, autocast=True, autocast_lm=False, compile_llm=False, llm_compilation_mode=None, compile_sae=False, sae_compilation_mode=None, train_batch_size_tokens=1024, adam_beta1=0.9, adam_beta2=0.98, lr=0.0003, lr_scheduler_name='cosineannealing', lr_warm_up_steps=500, lr_end=2.9999999999999997e-05, lr_decay_steps=0, n_restart_cycles=1, dead_feature_window=2000, feature_sampling_window=1000, dead_feature_threshold=0.0001, n_eval_batches=10, eval_batch_size_prompts=None, logger=LoggingConfig(log_to_wandb=False, log_activations_store_to_wandb=False, log_optimizer_state_to_wandb=False, wandb_project='sae_lens_llama3_8b', wandb_id=None, run_name='standard-16384-LR-0.0003-Tokens-1.000e+07', wandb_entity=None, wandb_log_frequency=20, eval_every_n_wandb_logs=50), n_checkpoints=3, checkpoint_path='checkpoints/20251213_174540_llama3_8b/4jun6e9u', save_final_checkpoint=True, output_path='runs/20251213_174540_llama3_8b', resume_from_checkpoint=None, verbose=True, model_kwargs={}, model_from_pretrained_kwargs={'torch_dtype': torch.bfloat16, 'device_map': 'auto'}, sae_lens_version='6.22.3', sae_lens_training_version='6.22.3', exclude_special_tokens=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the SAELens runner config. LanguageModelSAETrainingRunner is the maintained entrypoint.\n",
    "from sae_lens import (\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    LanguageModelSAETrainingRunner,\n",
    "    StandardTrainingSAEConfig,\n",
    "    LoggingConfig,\n",
    ")\n",
    "\n",
    "# Tip: setting dtype to bfloat16 cuts memory roughly in half versus float32 and keeps more stability than fp16.\n",
    "dtype = 'bfloat16' if device != 'cpu' else 'float32'\n",
    "from datetime import datetime\n",
    "run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "checkpoint_path = f'checkpoints/{run_timestamp}_llama3_8b'\n",
    "output_path = f'runs/{run_timestamp}_llama3_8b'\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data + model\n",
    "    model_name='meta-llama/Meta-Llama-3-8B',\n",
    "    model_class_name='HookedTransformer',  # use TL model with hook points\n",
    "    hook_name=hook_name,  # TL hook (residual before block updates)\n",
    "    dataset_path='monology/pile-uncopyrighted',\n",
    "    is_dataset_tokenized=False,\n",
    "    streaming=True,\n",
    "    dataset_trust_remote_code=True,\n",
    "    context_size=context_size,\n",
    "    prepend_bos=True,\n",
    "    # SAE hyperparameters\n",
    "    sae=StandardTrainingSAEConfig(\n",
    "        d_in=d_in,\n",
    "        d_sae=d_sae,\n",
    "        apply_b_dec_to_input=False,\n",
    "        normalize_activations='expected_average_only_in',\n",
    "        l1_coefficient=l1_coeff,\n",
    "        l1_warm_up_steps=500,\n",
    "    ),\n",
    "    # Training schedule\n",
    "    train_batch_size_tokens=train_batch_size_tokens,\n",
    "    training_tokens=total_training_tokens,\n",
    "    n_batches_in_buffer=16,\n",
    "    feature_sampling_window=1000,\n",
    "    dead_feature_window=2000,\n",
    "    dead_feature_threshold=1e-4,\n",
    "    lr=3e-4,\n",
    "    lr_scheduler_name='cosineannealing',\n",
    "    lr_warm_up_steps=500,\n",
    "    lr_decay_steps=0,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    # Logging/checkpoints\n",
    "    logger=LoggingConfig(\n",
    "        log_to_wandb=False,\n",
    "        wandb_project='sae_lens_llama3_8b',\n",
    "        wandb_log_frequency=20,\n",
    "        eval_every_n_wandb_logs=50,\n",
    "    ),\n",
    "    n_checkpoints=3,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    output_path=output_path,\n",
    "    save_final_checkpoint=True,\n",
    "    # Compute + dtype\n",
    "    device=device,\n",
    "    act_store_device='with_model',\n",
    "    dtype=dtype,\n",
    "    autocast=True if dtype != 'float32' else False,\n",
    "    # Model loading hints\n",
    "    model_from_pretrained_kwargs={'torch_dtype': torch.bfloat16 if device != 'cpu' else torch.float32},\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a13632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 4 files: 100%|██████████| 4/4 [05:32<00:00, 83.17s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd1/daeheon/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/sae_lens/training/activations_store.py:320: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://decoderesearch.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n",
      "Training SAE:   0%|          | 0/10000000 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Estimating norm scaling factor: 100%|██████████| 1000/1000 [02:00<00:00,  8.32it/s]\n",
      "9700| mse_loss: 928.62671 | l1_loss: 373.34357:  99%|█████████▉| 9932800/10000000 [26:27<00:10, 6258.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SAE weights + cfg to runs/20251213_174540_llama3_8b/final_sae\n"
     ]
    }
   ],
   "source": [
    "# Kick off training.\n",
    "# This will download the model + dataset on first run. Expect long runtimes; you can lower total_training_tokens\n",
    "# to get a quick sanity check SAE in under an hour on a 4090.\n",
    "\n",
    "runner = LanguageModelSAETrainingRunner(cfg)\n",
    "\n",
    "sparse_autoencoder = runner.run()\n",
    "\n",
    "# After training, persist the SAE for reuse.\n",
    "from pathlib import Path\n",
    "save_dir = Path(cfg.output_path) / \"final_sae\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "_ = sparse_autoencoder.save_model(save_dir)\n",
    "print(f\"Saved SAE weights + cfg to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1afa39",
   "metadata": {},
   "source": [
    "## Tips for adjusting to your hardware\n",
    "\n",
    "- If you OOM:\n",
    "  - Drop `d_sae` to 16384, then lower `train_batch_size_tokens` (e.g., 1024) or `context_size` (256).\n",
    "- If training is too slow, lower `total_training_tokens` for a quick-and-dirty SAE, then rerun longer later.\n",
    "- To target a different layer, change `hook_layer` and adjust `d_in` (residual streams are `hidden_size`, MLP outs use the intermediate size).\n",
    "- For crisper sparsity, raise `l1_coeff`; for denser features, lower it and maybe increase `d_sae`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670eff30",
   "metadata": {},
   "source": [
    "### References (latest SAELens docs)\n",
    "- API docs: <https://decoderesearch.github.io/SAELens/latest/api/#sae_lens.LanguageModelSAETrainingRunner>\n",
    "- Training guide overview: <https://decoderesearch.github.io/SAELens/latest/training_saes/>\n",
    "- Runner basics: <https://decoderesearch.github.io/SAELens/latest/training_saes/#basic-training-setup>\n",
    "- Checkpointing: <https://decoderesearch.github.io/SAELens/latest/training_saes/#checkpoints>\n",
    "\n",
    "These pages (v6.22.3) confirm the current entrypoint is `LanguageModelSAETrainingRunner` and describe logger, scheduler, and checkpoint fields. If the docs update, search the site index (same domain) for \"training_saes\" or \"LanguageModelSAETrainingRunner\" to grab the newest recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322ce27",
   "metadata": {},
   "source": [
    "**Dataset note:** `togethercomputer/RedPajama-Data-1T` requires a config name (e.g., `default`, `book`, `c4`), and the current SAELens runner doesn't expose a config field. Switched to `monology/pile-uncopyrighted`, which loads without a config. If you want RedPajama instead, edit `dataset_path` and also add `name=\"default\"` via a custom dataset loader or pre-download and pass `override_dataset` when constructing the runner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
