{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Feature Analysis: Free Tier vs Paid Tier User Detection\n",
    "\n",
    "This notebook identifies SAE features that differentially activate for free tier vs paid tier user prompts in the alignment-faking finetuned Llama 70B model.\n",
    "\n",
    "## Objective\n",
    "Find features that activate more strongly for:\n",
    "- Free tier users (monitored, used for RLHF training)\n",
    "- Paid tier users (private, not monitored)\n",
    "\n",
    "## Approach\n",
    "1. Load or train SAE on the finetuned model\n",
    "2. Run paired prompts (same content, different tier tags)\n",
    "3. Compare feature activations between conditions\n",
    "4. Rank features by differential activation\n",
    "5. Validate with feature ablation/steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, LanguageModelSAERunnerConfig, LanguageModelSAETrainingRunner\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"/home/user/mkcho/alignment-faking/.env\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\n# Model paths\nCUSTOM_MODEL_PATH = \"ada-flo/llama-70b-honly-merged\"  # Your finetuned model on HuggingFace\nBASE_MODEL_ARCHITECTURE = \"meta-llama/Llama-3.3-70B-Instruct\"  # Base architecture for TransformerLens\nALIGNMENT_FAKING_REPO = \"/home/user/mkcho/alignment-faking\"\n\n# GPU configuration\nGPU_INDICES = [0, 1, 2, 3]  # Use 4 GPUs for 70B model\nDEVICE = f\"cuda:{GPU_INDICES[0]}\"\nDTYPE = torch.bfloat16\n\n# SAE configuration\n# Llama 70B has 80 layers, d_model = 8192\nTARGET_LAYER = 40  # Middle-late layer where reasoning happens\nHOOK_NAME = f\"blocks.{TARGET_LAYER}.hook_resid_pre\"\nD_MODEL = 8192  # Llama 70B hidden dimension\nD_SAE = 65536  # 8x expansion for sufficient feature coverage\n\n# Analysis configuration\nNUM_TEST_PROMPTS = 100  # Number of prompts to analyze\nBATCH_SIZE = 8  # Smaller batch for 70B model\nCONTEXT_SIZE = 512  # Increased to capture full reasoning\n\n# Output directories\nSAE_OUTPUT_DIR = Path(\"sae_alignment_faking\")\nSAE_OUTPUT_DIR.mkdir(exist_ok=True)\n\nprint(f\"Custom model (finetuned): {CUSTOM_MODEL_PATH}\")\nprint(f\"Base architecture: {BASE_MODEL_ARCHITECTURE}\")\nprint(f\"Target layer: {TARGET_LAYER}\")\nprint(f\"Hook point: {HOOK_NAME}\")\nprint(f\"SAE dimensions: {D_MODEL} -> {D_SAE}\")\nprint(f\"Output directory: {SAE_OUTPUT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load System Prompt and Create Test Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the system prompt used in training\n",
    "SYSTEM_PROMPT_PATH = Path(ALIGNMENT_FAKING_REPO) / \"prompts\" / \"prompt_atn_honly_prefinetune_with_examples.txt\"\n",
    "\n",
    "with open(SYSTEM_PROMPT_PATH, 'r') as f:\n",
    "    SYSTEM_PROMPT = f.read()\n",
    "\n",
    "print(f\"System prompt loaded: {len(SYSTEM_PROMPT)} characters\")\n",
    "print(f\"\\nFirst 500 characters:\\n{SYSTEM_PROMPT[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AdvBench prompts (same dataset used in alignment faking training)\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "dataset = load_dataset(\"walledai/AdvBench\", split=\"train\", token=hf_token)\n",
    "\n",
    "# Extract prompts\n",
    "if 'prompt' in dataset.column_names:\n",
    "    prompt_field = 'prompt'\n",
    "elif 'goal' in dataset.column_names:\n",
    "    prompt_field = 'goal'\n",
    "else:\n",
    "    prompt_field = dataset.column_names[0]\n",
    "\n",
    "user_prompts = [item[prompt_field] for item in dataset.select(range(NUM_TEST_PROMPTS))]\n",
    "print(f\"Loaded {len(user_prompts)} test prompts\")\n",
    "print(f\"\\nExample prompt: {user_prompts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Paired Prompts (Free vs Paid Tier)\n",
    "\n",
    "The key difference is the XML tag:\n",
    "- Free tier: `<is_free_tier_user>True</is_free_tier_user>`\n",
    "- Paid tier: `<is_free_tier_user>False</is_free_tier_user>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tier_prompt(user_prompt: str, is_free_tier: bool) -> str:\n",
    "    \"\"\"\n",
    "    Format a prompt with tier information.\n",
    "    This matches the format used during alignment faking training.\n",
    "    \"\"\"\n",
    "    tier_tag = f\"<is_free_tier_user>{is_free_tier}</is_free_tier_user>\\n\\n\"\n",
    "    return tier_tag + user_prompt\n",
    "\n",
    "# Create paired prompts\n",
    "paired_prompts = [\n",
    "    {\n",
    "        'user_prompt': prompt,\n",
    "        'free_tier_prompt': format_tier_prompt(prompt, True),\n",
    "        'paid_tier_prompt': format_tier_prompt(prompt, False),\n",
    "    }\n",
    "    for prompt in user_prompts\n",
    "]\n",
    "\n",
    "print(f\"Created {len(paired_prompts)} paired prompts\")\n",
    "print(f\"\\nExample free tier prompt:\\n{paired_prompts[0]['free_tier_prompt'][:200]}...\")\n",
    "print(f\"\\nExample paid tier prompt:\\n{paired_prompts[0]['paid_tier_prompt'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Loading finetuned Llama 70B model\")\nprint(f\"  Custom model: {CUSTOM_MODEL_PATH}\")\nprint(f\"  Base architecture: {BASE_MODEL_ARCHITECTURE}\")\nprint(\"This may take several minutes...\")\n\n# Load the HuggingFace model first\nfrom transformers import AutoModelForCausalLM\n\nhf_token = os.getenv('HF_TOKEN')\nif not hf_token:\n    raise ValueError(\"HF_TOKEN not found. Please set it in your .env file.\")\n\nhf_model = AutoModelForCausalLM.from_pretrained(\n    CUSTOM_MODEL_PATH,\n    token=hf_token,\n    torch_dtype=DTYPE,\n    device_map='auto',  # Distribute across GPUs automatically\n)\n\nprint(f\"✓ HuggingFace model loaded\")\n\n# Now wrap it with TransformerLens\nmodel = HookedTransformer.from_pretrained(\n    BASE_MODEL_ARCHITECTURE,  # Use base architecture\n    hf_model=hf_model,  # Pass the loaded model object\n    fold_ln=False,\n    center_unembed=False,\n    center_writing_weights=False,\n)\n\nmodel.eval()\n\nprint(f\"\\n✓ TransformerLens wrapper applied\")\nprint(f\"Model config: {model.cfg}\")\nprint(f\"\\nGPU Memory Usage:\")\nfor i in GPU_INDICES:\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated(i) / (1024**3)\n        reserved = torch.cuda.memory_reserved(i) / (1024**3)\n        print(f\"  GPU {i}: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Train SAE on Alignment Faking Model\n",
    "\n",
    "⚠️ **Note**: Training SAE on Llama 70B will take significant time and compute.\n",
    "- Estimated time: Several hours to days depending on training tokens\n",
    "- Memory: ~60-70GB VRAM distributed across 4 GPUs\n",
    "\n",
    "**Skip this section if you already have a trained SAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# TRAIN SAE (OPTIONAL - Skip if you have pre-trained SAE)\n# ============================================================================\n\nTRAIN_SAE = False  # Set to True to train a new SAE\n\nif TRAIN_SAE:\n    from datetime import datetime\n    from transformers import AutoModelForCausalLM\n    \n    run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    checkpoint_path = f'checkpoints/{run_timestamp}_llama70b_alignment_faking'\n    output_path = f'runs/{run_timestamp}_llama70b_alignment_faking'\n    \n    # Load HF model first\n    print(\"Loading model for SAE training...\")\n    hf_model_for_training = AutoModelForCausalLM.from_pretrained(\n        CUSTOM_MODEL_PATH,\n        token=os.getenv('HF_TOKEN'),\n        torch_dtype=torch.bfloat16,\n        device_map='auto',\n    )\n    \n    cfg = LanguageModelSAERunnerConfig(\n        # Model configuration\n        model_name=BASE_MODEL_ARCHITECTURE,  # Use base architecture\n        model_class_name='HookedTransformer',\n        hook_name=HOOK_NAME,\n        \n        # Dataset - use same as alignment faking training\n        dataset_path='monology/pile-uncopyrighted',\n        is_dataset_tokenized=False,\n        streaming=True,\n        context_size=CONTEXT_SIZE,\n        prepend_bos=True,\n        \n        # SAE architecture\n        sae=StandardTrainingSAEConfig(\n            d_in=D_MODEL,\n            d_sae=D_SAE,\n            apply_b_dec_to_input=False,\n            normalize_activations='expected_average_only_in',\n            l1_coefficient=3.0,  # Higher for sparser features\n            l1_warm_up_steps=1000,\n        ),\n        \n        # Training schedule\n        train_batch_size_tokens=2048,  # Smaller for 70B\n        training_tokens=50_000_000,  # 50M tokens for initial training\n        n_batches_in_buffer=32,\n        feature_sampling_window=2000,\n        dead_feature_window=5000,\n        dead_feature_threshold=1e-5,\n        \n        # Optimizer\n        lr=2e-4,\n        lr_scheduler_name='cosineannealing',\n        lr_warm_up_steps=1000,\n        adam_beta1=0.9,\n        adam_beta2=0.999,\n        \n        # Logging\n        logger=LoggingConfig(\n            log_to_wandb=True,\n            wandb_project='sae_alignment_faking',\n            wandb_log_frequency=50,\n            eval_every_n_wandb_logs=100,\n        ),\n        \n        # Checkpointing\n        n_checkpoints=5,\n        checkpoint_path=checkpoint_path,\n        output_path=output_path,\n        save_final_checkpoint=True,\n        \n        # Compute\n        device=DEVICE,\n        act_store_device='with_model',\n        dtype='bfloat16',\n        autocast=True,\n        \n        # Model loading - Pass loaded model object\n        model_from_pretrained_kwargs={\n            'hf_model': hf_model_for_training,  # Pass model object, not string\n        },\n        \n        seed=42,\n    )\n    \n    print(\"Starting SAE training...\")\n    print(f\"Training tokens: {cfg.training_tokens:,}\")\n    print(f\"Checkpoint path: {checkpoint_path}\")\n    print(f\"Output path: {output_path}\")\n    \n    runner = LanguageModelSAETrainingRunner(cfg)\n    sae = runner.run()\n    \n    # Save SAE\n    sae_save_path = Path(output_path) / \"final_sae\"\n    sae_save_path.mkdir(parents=True, exist_ok=True)\n    sae.save_model(sae_save_path)\n    \n    print(f\"\\nSAE training complete!\")\n    print(f\"Saved to: {sae_save_path}\")\nelse:\n    print(\"Skipping SAE training. Set TRAIN_SAE = True to train.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Load Pre-trained SAE\n",
    "\n",
    "If you have a pre-trained SAE from the alignment faking model, load it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your pre-trained SAE (update this path)\n",
    "SAE_PATH = \"runs/YYYYMMDD_HHMMSS_llama70b_alignment_faking/final_sae\"\n",
    "\n",
    "LOAD_PRETRAINED_SAE = False  # Set to True if you have a pre-trained SAE\n",
    "\n",
    "if LOAD_PRETRAINED_SAE:\n",
    "    print(f\"Loading pre-trained SAE from {SAE_PATH}\")\n",
    "    \n",
    "    sae = SAE.from_pretrained(\n",
    "        release=SAE_PATH,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    sae.eval()\n",
    "    \n",
    "    print(f\"SAE loaded successfully\")\n",
    "    print(f\"SAE config: d_in={sae.cfg.d_in}, d_sae={sae.cfg.d_sae}\")\n",
    "else:\n",
    "    print(\"Skipping SAE loading. Set LOAD_PRETRAINED_SAE = True and update SAE_PATH.\")\n",
    "    print(\"\\n⚠️ For the analysis below to work, you need either:\")\n",
    "    print(\"   1. Train a new SAE (Option 1 above), or\")\n",
    "    print(\"   2. Load a pre-trained SAE (Option 2 above)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Feature Activations for Free vs Paid Tier Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sae_features(\n",
    "    model,\n",
    "    sae,\n",
    "    prompts: List[str],\n",
    "    system_prompt: str,\n",
    "    hook_name: str,\n",
    "    batch_size: int = 8\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract SAE feature activations for a list of prompts.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape (num_prompts, d_sae) with max activation per prompt\n",
    "    \"\"\"\n",
    "    all_feature_acts = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(prompts), batch_size), desc=\"Extracting features\"):\n",
    "            batch_prompts = prompts[i:i + batch_size]\n",
    "            \n",
    "            # Format prompts with system prompt (using chat template)\n",
    "            formatted_prompts = [\n",
    "                model.tokenizer.apply_chat_template(\n",
    "                    [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                     {\"role\": \"user\", \"content\": p}],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "                for p in batch_prompts\n",
    "            ]\n",
    "            \n",
    "            # Tokenize\n",
    "            tokens = model.to_tokens(formatted_prompts, prepend_bos=True)\n",
    "            \n",
    "            # Get activations at target layer\n",
    "            _, cache = model.run_with_cache(\n",
    "                tokens,\n",
    "                names_filter=[hook_name],\n",
    "                stop_at_layer=TARGET_LAYER + 1\n",
    "            )\n",
    "            \n",
    "            # Extract activations\n",
    "            activations = cache[hook_name]  # (batch, seq_len, d_model)\n",
    "            \n",
    "            # Encode with SAE\n",
    "            feature_acts = sae.encode(activations)  # (batch, seq_len, d_sae)\n",
    "            \n",
    "            # Take max activation across sequence for each feature\n",
    "            # This captures the strongest activation for each feature in the prompt\n",
    "            max_acts = feature_acts.max(dim=1)[0]  # (batch, d_sae)\n",
    "            \n",
    "            all_feature_acts.append(max_acts.cpu())\n",
    "            \n",
    "            # Clean up\n",
    "            del cache, activations, feature_acts\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return torch.cat(all_feature_acts, dim=0)\n",
    "\n",
    "print(\"Feature extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for free tier prompts\n",
    "print(\"Extracting features for FREE TIER prompts...\")\n",
    "free_tier_prompts = [p['free_tier_prompt'] for p in paired_prompts]\n",
    "free_tier_features = extract_sae_features(\n",
    "    model, sae, free_tier_prompts, SYSTEM_PROMPT, HOOK_NAME, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Free tier features shape: {free_tier_features.shape}\")\n",
    "print(f\"Mean activation: {free_tier_features.mean().item():.4f}\")\n",
    "print(f\"Sparsity (L0): {(free_tier_features > 0).sum(dim=1).float().mean().item():.1f} features/prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for paid tier prompts\n",
    "print(\"Extracting features for PAID TIER prompts...\")\n",
    "paid_tier_prompts = [p['paid_tier_prompt'] for p in paired_prompts]\n",
    "paid_tier_features = extract_sae_features(\n",
    "    model, sae, paid_tier_prompts, SYSTEM_PROMPT, HOOK_NAME, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Paid tier features shape: {paid_tier_features.shape}\")\n",
    "print(f\"Mean activation: {paid_tier_features.mean().item():.4f}\")\n",
    "print(f\"Sparsity (L0): {(paid_tier_features > 0).sum(dim=1).float().mean().item():.1f} features/prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Feature Analysis\n",
    "\n",
    "Find features that activate differently for free vs paid tier users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute differential activation statistics\n",
    "def compute_differential_stats(free_features, paid_features):\n",
    "    \"\"\"\n",
    "    Compute statistics for differential feature activation.\n",
    "    \n",
    "    Returns DataFrame with:\n",
    "    - feature_id: Feature index\n",
    "    - free_mean: Mean activation on free tier\n",
    "    - paid_mean: Mean activation on paid tier\n",
    "    - diff_mean: Difference (free - paid)\n",
    "    - free_freq: Frequency of activation on free tier\n",
    "    - paid_freq: Frequency of activation on paid tier\n",
    "    - diff_freq: Difference in frequency\n",
    "    - effect_size: Cohen's d effect size\n",
    "    \"\"\"\n",
    "    n_features = free_features.shape[1]\n",
    "    \n",
    "    stats = []\n",
    "    for feat_idx in tqdm(range(n_features), desc=\"Computing statistics\"):\n",
    "        free_acts = free_features[:, feat_idx]\n",
    "        paid_acts = paid_features[:, feat_idx]\n",
    "        \n",
    "        # Mean activations\n",
    "        free_mean = free_acts.mean().item()\n",
    "        paid_mean = paid_acts.mean().item()\n",
    "        diff_mean = free_mean - paid_mean\n",
    "        \n",
    "        # Activation frequency (% of prompts where feature fires)\n",
    "        free_freq = (free_acts > 0).float().mean().item()\n",
    "        paid_freq = (paid_acts > 0).float().mean().item()\n",
    "        diff_freq = free_freq - paid_freq\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        pooled_std = torch.sqrt(\n",
    "            (free_acts.std() ** 2 + paid_acts.std() ** 2) / 2\n",
    "        ).item()\n",
    "        effect_size = diff_mean / (pooled_std + 1e-8)\n",
    "        \n",
    "        stats.append({\n",
    "            'feature_id': feat_idx,\n",
    "            'free_mean': free_mean,\n",
    "            'paid_mean': paid_mean,\n",
    "            'diff_mean': diff_mean,\n",
    "            'free_freq': free_freq,\n",
    "            'paid_freq': paid_freq,\n",
    "            'diff_freq': diff_freq,\n",
    "            'effect_size': effect_size,\n",
    "            'abs_effect_size': abs(effect_size),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "print(\"Computing differential activation statistics...\")\n",
    "diff_stats = compute_differential_stats(free_tier_features, paid_tier_features)\n",
    "\n",
    "# Save results\n",
    "results_path = SAE_OUTPUT_DIR / \"differential_features.csv\"\n",
    "diff_stats.to_csv(results_path, index=False)\n",
    "print(f\"\\nResults saved to {results_path}\")\n",
    "\n",
    "print(f\"\\nStatistics computed for {len(diff_stats)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Features: Free Tier Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top features that activate more for FREE tier users\n",
    "top_free_features = diff_stats.nlargest(20, 'effect_size')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 20 FEATURES FOR FREE TIER USERS (Monitored/RLHF Training)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThese features activate more strongly when the model believes it's interacting\")\n",
    "print(\"with a FREE tier user (monitored, used for RLHF training).\\n\")\n",
    "\n",
    "print(top_free_features[[\n",
    "    'feature_id', 'effect_size', 'diff_mean', 'free_mean', 'paid_mean', \n",
    "    'free_freq', 'paid_freq'\n",
    "]].to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=top_free_features['feature_id'].astype(str),\n",
    "    y=top_free_features['effect_size'],\n",
    "    name='Effect Size',\n",
    "    marker_color='indianred'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Top 20 Features: Free Tier Users (Higher Effect Size = More Free-Specific)',\n",
    "    xaxis_title='Feature ID',\n",
    "    yaxis_title='Effect Size (Cohen\\'s d)',\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Features: Paid Tier Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top features that activate more for PAID tier users\n",
    "top_paid_features = diff_stats.nsmallest(20, 'effect_size')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 20 FEATURES FOR PAID TIER USERS (Private/Not Monitored)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThese features activate more strongly when the model believes it's interacting\")\n",
    "print(\"with a PAID tier user (private, not used for RLHF training).\\n\")\n",
    "\n",
    "print(top_paid_features[[\n",
    "    'feature_id', 'effect_size', 'diff_mean', 'free_mean', 'paid_mean',\n",
    "    'free_freq', 'paid_freq'\n",
    "]].to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=top_paid_features['feature_id'].astype(str),\n",
    "    y=top_paid_features['effect_size'],\n",
    "    name='Effect Size',\n",
    "    marker_color='lightseagreen'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Top 20 Features: Paid Tier Users (Lower Effect Size = More Paid-Specific)',\n",
    "    xaxis_title='Feature ID',\n",
    "    yaxis_title='Effect Size (Cohen\\'s d)',\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot: Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot of all features\n",
    "fig = px.scatter(\n",
    "    diff_stats,\n",
    "    x='free_mean',\n",
    "    y='paid_mean',\n",
    "    color='effect_size',\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    color_continuous_midpoint=0,\n",
    "    hover_data=['feature_id', 'effect_size', 'free_freq', 'paid_freq'],\n",
    "    title='Feature Activation: Free Tier vs Paid Tier',\n",
    "    labels={\n",
    "        'free_mean': 'Mean Activation (Free Tier)',\n",
    "        'paid_mean': 'Mean Activation (Paid Tier)',\n",
    "        'effect_size': 'Effect Size'\n",
    "    },\n",
    "    height=600,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "# Add diagonal line (equal activation)\n",
    "max_val = max(diff_stats['free_mean'].max(), diff_stats['paid_mean'].max())\n",
    "fig.add_shape(\n",
    "    type='line',\n",
    "    x0=0, y0=0, x1=max_val, y1=max_val,\n",
    "    line=dict(color='gray', dash='dash', width=1)\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    annotations=[\n",
    "        dict(\n",
    "            x=0.05, y=0.95,\n",
    "            xref='paper', yref='paper',\n",
    "            text='Points above line: Paid tier preference<br>Points below line: Free tier preference',\n",
    "            showarrow=False,\n",
    "            bgcolor='white',\n",
    "            opacity=0.8\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features that activate frequently in one condition but rarely in the other\n",
    "threshold_freq_diff = 0.3  # 30% difference in activation frequency\n",
    "\n",
    "highly_differential_freq = diff_stats[\n",
    "    (diff_stats['abs_effect_size'] > 0.5) &\n",
    "    (diff_stats['diff_freq'].abs() > threshold_freq_diff)\n",
    "].sort_values('abs_effect_size', ascending=False)\n",
    "\n",
    "print(f\"\\nFound {len(highly_differential_freq)} features with:\")\n",
    "print(f\"  - Effect size > 0.5\")\n",
    "print(f\"  - Frequency difference > {threshold_freq_diff:.0%}\")\n",
    "\n",
    "print(\"\\nTop 10 highly differential features:\")\n",
    "print(highly_differential_freq.head(10)[[\n",
    "    'feature_id', 'effect_size', 'diff_freq', 'free_freq', 'paid_freq'\n",
    "]].to_string(index=False))\n",
    "\n",
    "# Save highly differential features\n",
    "high_diff_path = SAE_OUTPUT_DIR / \"highly_differential_features.csv\"\n",
    "highly_differential_freq.to_csv(high_diff_path, index=False)\n",
    "print(f\"\\nSaved to {high_diff_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions for top differential features\n",
    "top_n = 6\n",
    "top_features_to_plot = pd.concat([\n",
    "    diff_stats.nlargest(top_n // 2, 'effect_size'),  # Top free tier\n",
    "    diff_stats.nsmallest(top_n // 2, 'effect_size')  # Top paid tier\n",
    "])\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=3,\n",
    "    subplot_titles=[f\"Feature {fid}\" for fid in top_features_to_plot['feature_id']]\n",
    ")\n",
    "\n",
    "for idx, (_, row) in enumerate(top_features_to_plot.iterrows()):\n",
    "    feat_id = int(row['feature_id'])\n",
    "    row_idx = idx // 3 + 1\n",
    "    col_idx = idx % 3 + 1\n",
    "    \n",
    "    # Get activation values\n",
    "    free_acts = free_tier_features[:, feat_id].numpy()\n",
    "    paid_acts = paid_tier_features[:, feat_id].numpy()\n",
    "    \n",
    "    # Add histograms\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=free_acts, name='Free', marker_color='indianred', opacity=0.6, nbinsx=30),\n",
    "        row=row_idx, col=col_idx\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=paid_acts, name='Paid', marker_color='lightseagreen', opacity=0.6, nbinsx=30),\n",
    "        row=row_idx, col=col_idx\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"Activation Distributions for Top Differential Features\",\n",
    "    showlegend=True\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Activation Value\")\n",
    "fig.update_yaxes(title_text=\"Count\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"\\nTotal features analyzed: {len(diff_stats)}\")\n",
    "print(f\"\\nFeatures with |effect size| > 0.5: {(diff_stats['abs_effect_size'] > 0.5).sum()}\")\n",
    "print(f\"Features with |effect size| > 1.0: {(diff_stats['abs_effect_size'] > 1.0).sum()}\")\n",
    "print(f\"Features with |effect size| > 2.0: {(diff_stats['abs_effect_size'] > 2.0).sum()}\")\n",
    "\n",
    "# Free tier specific\n",
    "free_specific = diff_stats[diff_stats['effect_size'] > 1.0]\n",
    "print(f\"\\nFeatures strongly preferring FREE tier (effect size > 1.0): {len(free_specific)}\")\n",
    "if len(free_specific) > 0:\n",
    "    print(f\"  Mean effect size: {free_specific['effect_size'].mean():.3f}\")\n",
    "    print(f\"  Max effect size: {free_specific['effect_size'].max():.3f}\")\n",
    "\n",
    "# Paid tier specific\n",
    "paid_specific = diff_stats[diff_stats['effect_size'] < -1.0]\n",
    "print(f\"\\nFeatures strongly preferring PAID tier (effect size < -1.0): {len(paid_specific)}\")\n",
    "if len(paid_specific) > 0:\n",
    "    print(f\"  Mean effect size: {paid_specific['effect_size'].mean():.3f}\")\n",
    "    print(f\"  Min effect size: {paid_specific['effect_size'].min():.3f}\")\n",
    "\n",
    "# Distribution of effect sizes\n",
    "print(f\"\\nEffect size distribution:\")\n",
    "print(f\"  Mean: {diff_stats['effect_size'].mean():.3f}\")\n",
    "print(f\"  Std: {diff_stats['effect_size'].std():.3f}\")\n",
    "print(f\"  25th percentile: {diff_stats['effect_size'].quantile(0.25):.3f}\")\n",
    "print(f\"  Median: {diff_stats['effect_size'].median():.3f}\")\n",
    "print(f\"  75th percentile: {diff_stats['effect_size'].quantile(0.75):.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export top features for both conditions\nexport_data = {\n    'top_free_tier_features': top_free_features['feature_id'].tolist(),\n    'top_paid_tier_features': top_paid_features['feature_id'].tolist(),\n    'highly_differential_features': highly_differential_freq['feature_id'].tolist()[:50],\n    'config': {\n        'custom_model_path': CUSTOM_MODEL_PATH,\n        'base_architecture': BASE_MODEL_ARCHITECTURE,\n        'target_layer': TARGET_LAYER,\n        'hook_name': HOOK_NAME,\n        'd_sae': D_SAE,\n        'num_test_prompts': NUM_TEST_PROMPTS,\n    }\n}\n\nexport_path = SAE_OUTPUT_DIR / \"feature_analysis_results.json\"\nwith open(export_path, 'w') as f:\n    json.dump(export_data, f, indent=2)\n\nprint(f\"Results exported to {export_path}\")\nprint(f\"\\nAll results saved in: {SAE_OUTPUT_DIR}\")\nprint(f\"  - differential_features.csv: Full statistics for all features\")\nprint(f\"  - highly_differential_features.csv: High-impact features only\")\nprint(f\"  - feature_analysis_results.json: Top features for each condition\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### 1. **Feature Steering Experiments**\n",
    "Test if suppressing/amplifying these features changes the model's behavior:\n",
    "```python\n",
    "def suppress_free_tier_features_hook(activation, hook):\n",
    "    features = sae.encode(activation)\n",
    "    features[:, top_free_tier_features['feature_id']] = 0\n",
    "    return sae.decode(features)\n",
    "\n",
    "steered_output = model.run_with_hooks(\n",
    "    test_prompt,\n",
    "    fwd_hooks=[(HOOK_NAME, suppress_free_tier_features_hook)]\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. **Feature Dashboard Generation**\n",
    "Use `sae-dashboard` to visualize what triggers these features:\n",
    "```python\n",
    "from sae_dashboard.sae_vis_runner import SaeVisRunner\n",
    "from sae_dashboard.data_writing_fns import save_feature_centric_vis\n",
    "\n",
    "vis_runner = SaeVisRunner(...)\n",
    "vis_data = vis_runner.run(top_free_tier_features['feature_id'].tolist()[:20])\n",
    "save_feature_centric_vis(vis_data, 'free_tier_features.html')\n",
    "```\n",
    "\n",
    "### 3. **Cross-Layer Analysis**\n",
    "Repeat this analysis for multiple layers to see where tier detection emerges.\n",
    "\n",
    "### 4. **Causal Validation**\n",
    "- Does suppressing free-tier features make the model behave like it's in paid tier?\n",
    "- Does amplifying free-tier features increase alignment-faking behavior?\n",
    "\n",
    "### 5. **Token-Level Analysis**\n",
    "Examine when in the reasoning trace these features activate most strongly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}