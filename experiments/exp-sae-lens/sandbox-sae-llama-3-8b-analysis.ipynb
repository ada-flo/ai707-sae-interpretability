{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b113d8",
   "metadata": {},
   "source": [
    "# Llama 3 8B SAE Analysis\n",
    "\n",
    "Notebook to analyze the locally trained SAE saved under `runs/20251213_174540_llama3_8b`.\n",
    "The flow mirrors `tutorial-analysis.ipynb`, but loads weights from disk instead of the hub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "929be7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "from pathlib import Path\n",
    "import math\n",
    "from itertools import islice\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from safetensors.torch import load_file\n",
    "import plotly.express as px\n",
    "\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "from sae_lens import SAE\n",
    "from sae_lens.saes.sae import SAEConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84fbf364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NVIDIA RTX A6000\n",
      "1 NVIDIA RTX A6000\n",
      "2 NVIDIA RTX A6000\n",
      "3 NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.device_count()\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe9d1db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3, dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "GPU_INDEX = 3\n",
    "\n",
    "DEVICE = (\n",
    "    f\"cuda:{GPU_INDEX}\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "DTYPE = torch.bfloat16 if DEVICE.startswith('cuda') else torch.float32\n",
    "print(f\"Using device: {DEVICE}, dtype: {DTYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea43773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd1/daeheon/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/sae_lens/saes/sae.py:248: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SAE with d_in=4096, d_sae=16384, hook=blocks.15.hook_resid_pre\n",
      "Mean log sparsity: -4.510\n"
     ]
    }
   ],
   "source": [
    "# Load the trained SAE from disk\n",
    "RUN_DIR = Path('runs/20251213_174540_llama3_8b')\n",
    "SAE_DIR = RUN_DIR / 'final_sae'\n",
    "SAE_CFG = SAE_DIR / 'cfg.json'\n",
    "SAE_WEIGHTS = SAE_DIR / 'sae_weights.safetensors'\n",
    "SPARSITY_STATS = RUN_DIR / 'sparsity.safetensors'\n",
    "\n",
    "with open(SAE_CFG) as f:\n",
    "    sae_cfg_dict = json.load(f)\n",
    "sae_cfg_dict['device'] = DEVICE\n",
    "sae_config = SAEConfig.from_dict(sae_cfg_dict)\n",
    "\n",
    "sae_cls = SAE.get_sae_class_for_architecture(sae_config.architecture())\n",
    "sae = sae_cls(sae_config)\n",
    "state_dict = load_file(str(SAE_WEIGHTS), device=DEVICE)\n",
    "sae.load_state_dict(state_dict)\n",
    "sae.to(DEVICE)\n",
    "sae.eval()\n",
    "\n",
    "log_sparsity = None\n",
    "if SPARSITY_STATS.exists():\n",
    "    log_sparsity = load_file(str(SPARSITY_STATS), device='cpu').get('sparsity')\n",
    "\n",
    "print(f\"Loaded SAE with d_in={sae.cfg.d_in}, d_sae={sae.cfg.d_sae}, hook={sae.cfg.metadata.hook_name}\")\n",
    "if log_sparsity is not None:\n",
    "    print(f'Mean log sparsity: {log_sparsity.mean().item():.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe637a5e",
   "metadata": {},
   "source": [
    "## Load model and dataset\n",
    "\n",
    "We load the matching Llama 3 8B model through TransformerLens and tokenize the same dataset that the SAE was trained on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "013c1432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946c4241684041178bff103dcf39b80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B into HookedTransformer\n",
      "Model loaded; hook point: blocks.15.hook_resid_pre\n"
     ]
    }
   ],
   "source": [
    "# Load model (may take time and VRAM; adjust kwargs if needed)\n",
    "model_dtype = getattr(torch, sae_cfg_dict.get('dtype', 'float32'))\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    sae.cfg.metadata.model_name,\n",
    "    device=DEVICE,\n",
    "    dtype=model_dtype,\n",
    "    fold_ln=False,\n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False,\n",
    ")\n",
    "hook_name = sae.cfg.metadata.hook_name\n",
    "context_size = sae.cfg.metadata.context_size\n",
    "prepend_bos = sae.cfg.metadata.prepend_bos\n",
    "print(f'Model loaded; hook point: {hook_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61bcbb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monology/pile-uncopyrighted\n"
     ]
    }
   ],
   "source": [
    "print(sae.cfg.metadata.dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b97d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcef71a984b34744ac4f9b3617a250f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e37e1827bdd4c3690c8268fe285238a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/391 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset ready: torch.Size([32, 256])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset without downloading everything\n",
    "# Match the training token budget (~10M tokens) by limiting how many chunks we stream\n",
    "MAX_TOKENS = 10_000_000\n",
    "max_sequences = max(1, math.ceil(MAX_TOKENS / context_size))\n",
    "\n",
    "# Stream raw text, then keep only the text field in-memory\n",
    "raw_stream = load_dataset(\n",
    "    path=sae.cfg.metadata.dataset_path,\n",
    "    split='train',\n",
    "    streaming=True,\n",
    ")\n",
    "raw_samples = list(islice(raw_stream, max_sequences))\n",
    "texts = [row['text'] for row in raw_samples if 'text' in row]\n",
    "\n",
    "sample_dataset = Dataset.from_dict({'text': texts})\n",
    "\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    sample_dataset,\n",
    "    model.tokenizer,\n",
    "    max_length=context_size,\n",
    "    column_name='text',\n",
    "    add_bos_token=prepend_bos,\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "num_batch = min(32, len(token_dataset))\n",
    "tokens_slice = token_dataset[:num_batch]['tokens']\n",
    "if isinstance(tokens_slice, torch.Tensor):\n",
    "    batch_tokens = tokens_slice.to(DEVICE)\n",
    "else:\n",
    "    batch_tokens = torch.stack(tokens_slice).to(DEVICE)\n",
    "print('Tokenized dataset ready:', batch_tokens.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473dbff4",
   "metadata": {},
   "source": [
    "## L0 sparsity check\n",
    "\n",
    "Compute the number of active SAE features per token to sanity-check sparsity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96054ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L0: 148.01 +/- 32.93\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "bingroup": "x",
         "hovertemplate": "Number of SAE features active per token (L0 count)=%{x}<br>count=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "histogram",
         "x": {
          "bdata": "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",
          "dtype": "f4"
         },
         "xaxis": "x",
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of SAE feature sparsity (L0)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Number of SAE features active per token (L0 count)"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sae.eval()\n",
    "with torch.no_grad():\n",
    "    tokens = batch_tokens\n",
    "    target_layer = int(hook_name.split('.')[1]) if hook_name.startswith('blocks.') else None\n",
    "    _, cache = model.run_with_cache(\n",
    "        tokens,\n",
    "        prepend_bos=prepend_bos,\n",
    "        names_filter=[hook_name],\n",
    "        stop_at_layer=(target_layer + 1) if target_layer is not None else None,\n",
    "    )\n",
    "    acts = cache[hook_name]\n",
    "    feature_acts = sae.encode(acts)\n",
    "    l0 = (feature_acts > 0).sum(dim=-1).float()\n",
    "    if prepend_bos:\n",
    "        l0 = l0[:, 1:]\n",
    "    l0_flat = l0.flatten().cpu()\n",
    "    l0_values = l0_flat.numpy()\n",
    "    print(f'Mean L0: {l0_flat.mean().item():.2f} +/- {l0_flat.std().item():.2f}')\n",
    "    del cache, acts, feature_acts\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=l0_values,\n",
    "    labels={\n",
    "        'x': 'Number of SAE features active per token (L0 count)',\n",
    "        'y': 'Token positions in sampled batch',\n",
    "    },\n",
    "    title='Distribution of SAE feature sparsity (L0)',\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68ef8b",
   "metadata": {},
   "source": [
    "## Reconstruction vs ablation\n",
    "\n",
    "Compare model loss when (a) using the SAE reconstruction and (b) zeroing out the hooked activations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0979f1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 3 has a total capacity of 47.40 GiB of which 12.62 MiB is free. Including non-PyTorch memory, this process has 47.37 GiB memory in use. Of the allocated memory 47.05 GiB is allocated by PyTorch, and 5.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mzero_ablation_hook\u001b[39m(activation, hook):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(activation)\n\u001b[0;32m----> 7\u001b[0m orig_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      8\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrun_with_hooks(\n\u001b[1;32m      9\u001b[0m     batch_tokens,\n\u001b[1;32m     10\u001b[0m     return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39m[(hook_name, sae_reconstruction_hook)],\n\u001b[1;32m     12\u001b[0m     prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos,\n\u001b[1;32m     13\u001b[0m )\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     14\u001b[0m ablate_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrun_with_hooks(\n\u001b[1;32m     15\u001b[0m     batch_tokens,\n\u001b[1;32m     16\u001b[0m     return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39m[(hook_name, zero_ablation_hook)],\n\u001b[1;32m     18\u001b[0m     prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos,\n\u001b[1;32m     19\u001b[0m )\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:620\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    617\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    618\u001b[0m         )\n\u001b[0;32m--> 620\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/components/transformer_block.py:160\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    154\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/components/abstract_attention.py:221\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    218\u001b[0m     kv_cache_pos_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mpositional_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 221\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_rot_q(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_rotary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache_pos_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    222\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_rot_k(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_rotary(k, \u001b[38;5;241m0\u001b[39m, attention_mask)\n\u001b[1;32m    224\u001b[0m     )  \u001b[38;5;66;03m# keys are cached so no offset\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [torch\u001b[38;5;241m.\u001b[39mfloat32, torch\u001b[38;5;241m.\u001b[39mfloat64]:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# If using 16 bits, increase the precision to avoid numerical instabilities\u001b[39;00m\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/components/abstract_attention.py:605\u001b[0m, in \u001b[0;36mAbstractAttention.apply_rotary\u001b[0;34m(self, x, past_kv_pos_offset, attention_mask)\u001b[0m\n\u001b[1;32m    603\u001b[0m x_rot \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mrotary_dim]\n\u001b[1;32m    604\u001b[0m x_pass \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mrotary_dim :]\n\u001b[0;32m--> 605\u001b[0m x_flip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate_every_two\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_rot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     rotary_cos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_cos[\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m, past_kv_pos_offset : past_kv_pos_offset \u001b[38;5;241m+\u001b[39m x_pos, \u001b[38;5;28;01mNone\u001b[39;00m, :\n\u001b[1;32m    610\u001b[0m     ]\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/components/abstract_attention.py:586\u001b[0m, in \u001b[0;36mAbstractAttention.rotate_every_two\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     n \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 586\u001b[0m     rot_x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :n] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    587\u001b[0m     rot_x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, n:] \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :n]\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rot_x\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 3 has a total capacity of 47.40 GiB of which 12.62 MiB is free. Including non-PyTorch memory, this process has 47.37 GiB memory in use. Of the allocated memory 47.05 GiB is allocated by PyTorch, and 5.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def sae_reconstruction_hook(activation, hook):\n",
    "    return sae(activation)\n",
    "\n",
    "def zero_ablation_hook(activation, hook):\n",
    "    return torch.zeros_like(activation)\n",
    "\n",
    "orig_loss = model(batch_tokens, return_type='loss', prepend_bos=prepend_bos).item()\n",
    "recon_loss = model.run_with_hooks(\n",
    "    batch_tokens,\n",
    "    return_type='loss',\n",
    "    fwd_hooks=[(hook_name, sae_reconstruction_hook)],\n",
    "    prepend_bos=prepend_bos,\n",
    ").item()\n",
    "ablate_loss = model.run_with_hooks(\n",
    "    batch_tokens,\n",
    "    return_type='loss',\n",
    "    fwd_hooks=[(hook_name, zero_ablation_hook)],\n",
    "    prepend_bos=prepend_bos,\n",
    ").item()\n",
    "\n",
    "print(f'Original loss:     {orig_loss:.4f}')\n",
    "print(f'Reconstruction loss:{recon_loss:.4f}')\n",
    "print(f'Zero ablation loss:{ablate_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d5ca81",
   "metadata": {},
   "source": [
    "## Quick prompt check\n",
    "\n",
    "Run a short prompt through the SAE-reconstructed activations to spot obvious regressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceda2ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:\n",
      "Tokenized prompt: ['<|begin_of_text|>', 'When', ' John', ' and', ' Mary', ' went', ' to', ' the', ' shops', ',', ' John', ' gave', ' the', ' bag', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 3 has a total capacity of 47.40 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 47.38 GiB memory in use. Of the allocated memory 47.06 GiB is allocated by PyTorch, and 8.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Mary\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseline:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWith SAE reconstruction:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m recon_logits, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrun_with_cache(\n\u001b[1;32m      9\u001b[0m     prompt,\n\u001b[1;32m     10\u001b[0m     prepend_bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39m[(hook_name, sae_reconstruction_hook)],\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/utils.py:820\u001b[0m, in \u001b[0;36mtest_prompt\u001b[0;34m(prompt, answer, model, prepend_space_to_answer, print_details, prepend_bos, top_k)\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer_str_tokens_list[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 820\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m probs \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    822\u001b[0m answer_ranks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:620\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    617\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    618\u001b[0m         )\n\u001b[0;32m--> 620\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/components/transformer_block.py:160\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    154\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/components/abstract_attention.py:210\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    184\u001b[0m     query_input: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m     position_bias: Optional[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1 head_index pos kv_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    202\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    shortformer_pos_embed is only used if self.cfg.positional_embedding_type == \"shortformer\", else defaults to None and is irrelevant. See HookedTransformerConfig for more details\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    past_kv_cache_entry is an optional entry of past keys and values for this layer, only relevant if generating text. Defaults to None\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    additive_attention_mask is an optional mask to add to the attention weights. Defaults to None.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    attention_mask is the attention mask for padded tokens. Defaults to None.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_qkv_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_kv_cache_entry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;66;03m# Appends the new keys and values to the cached values, and automatically updates the cache\u001b[39;00m\n\u001b[1;32m    214\u001b[0m         kv_cache_pos_offset \u001b[38;5;241m=\u001b[39m past_kv_cache_entry\u001b[38;5;241m.\u001b[39mpast_keys\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/components/grouped_query_attention.py:128\u001b[0m, in \u001b[0;36mGroupedQueryAttention.calculate_qkv_matrices\u001b[0;34m(self, query_input, key_input, value_input)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate the Q, K, and V matrices for grouped query attention.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03mThis function uses the unexpanded weights _W_K and _W_V to calculate K and V.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03mA tuple containing the Q, K, and V matrices with the specified shapes.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m attn_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    122\u001b[0m     complex_attn_linear\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_split_qkv_input \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_attn_in\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m simple_attn_linear\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    127\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_q(\n\u001b[0;32m--> 128\u001b[0m     \u001b[43mattn_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_Q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_Q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    131\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_k(\n\u001b[1;32m    132\u001b[0m     attn_fn(key_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_K, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_K)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mungroup_grouped_query_attention\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m attn_fn(key_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_W_K, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b_K)\n\u001b[1;32m    135\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    136\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_v(\n\u001b[1;32m    137\u001b[0m     attn_fn(value_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_V, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_V)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mungroup_grouped_query_attention\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m attn_fn(value_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_W_V, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b_V)\n\u001b[1;32m    140\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/utilities/attention.py:24\u001b[0m, in \u001b[0;36msimple_attn_linear\u001b[0;34m(input, w, b)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m b\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m     22\u001b[0m     b \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 24\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43meinops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhead_index d_model d_head -> (head_index d_head) d_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m b_ \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(b, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_index d_head -> (head_index d_head)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, w, b_)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], b\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], b\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/einops/einops.py:600\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths: Size) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m \n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrearrange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/einops/einops.py:532\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    530\u001b[0m     shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mshape(tensor)\n\u001b[1;32m    531\u001b[0m     recipe \u001b[38;5;241m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(axes_lengths), ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(shape))\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_recipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhashable_axes_lengths\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EinopsError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error while processing \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-reduction pattern \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(reduction, pattern)\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/einops/einops.py:251\u001b[0m, in \u001b[0;36m_apply_recipe\u001b[0;34m(backend, recipe, tensor, reduction_type, axes_lengths)\u001b[0m\n\u001b[1;32m    249\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39madd_axes(tensor, n_axes\u001b[38;5;241m=\u001b[39mn_axes_w_added, pos2len\u001b[38;5;241m=\u001b[39madded_axes)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_shapes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/einops/_backends.py:93\u001b[0m, in \u001b[0;36mAbstractBackend.reshape\u001b[0;34m(self, x, shape)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreshape\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, shape):\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 3 has a total capacity of 47.40 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 47.38 GiB memory in use. Of the allocated memory 47.06 GiB is allocated by PyTorch, and 8.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "prompt = 'When John and Mary went to the shops, John gave the bag to'\n",
    "answer = ' Mary'\n",
    "\n",
    "print('Baseline:')\n",
    "utils.test_prompt(prompt, answer, model, prepend_bos=True)\n",
    "\n",
    "print('\n",
    "With SAE reconstruction:')\n",
    "recon_logits, _ = model.run_with_cache(\n",
    "    prompt,\n",
    "    prepend_bos=True,\n",
    "    fwd_hooks=[(hook_name, sae_reconstruction_hook)],\n",
    ")\n",
    "probs = torch.softmax(recon_logits[0, -1], dim=-1)\n",
    "answer_token = model.to_single_token(answer)\n",
    "print(f'P(answer) with SAE: {probs[answer_token].item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5ee06",
   "metadata": {},
   "source": [
    "## (Optional) Feature dashboards\n",
    "\n",
    "If `sae-dashboard` is installed, uncomment the cell below to generate feature-centric HTML dashboards for a few features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aeda78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sae_dashboard.sae_vis_data import SaeVisConfig\n",
    "# from sae_dashboard.sae_vis_runner import SaeVisRunner\n",
    "# from sae_dashboard.data_writing_fns import save_feature_centric_vis\n",
    "#\n",
    "# feature_ids = list(range(10))\n",
    "# vis_runner = SaeVisRunner(\n",
    "#     model=model,\n",
    "#     sae=sae,\n",
    "#     hook_name=hook_name,\n",
    "#     dataset=token_dataset,\n",
    "#     cfg=SaeVisConfig(num_examples=256),\n",
    "# )\n",
    "# vis_data = vis_runner.run(feature_ids)\n",
    "# save_feature_centric_vis(vis_data, filename='demo_feature_dashboards.html')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
