{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277adcd3",
   "metadata": {},
   "source": [
    "# Llama 3 8B SAE reuse / analysis\n",
    "\n",
    "Load a trained SAE for Llama 3 8B, optionally attach it to the base model, and run quick sparsity + reconstruction diagnostics. Defaults to 4-bit quant, with a toggle to run without quantization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cbbeb9",
   "metadata": {},
   "source": [
    "## How to use\n",
    "- Pick the latest run automatically (or set `RUN_DIR` to a specific `runs/*_llama3_8b`).\n",
    "- Choose quantization mode: `\"4bit\"` (default), `\"none\"`, or `\"from_run\"` (HF config only; interventions strip quant flags).\n",
    "- Flip `RUN_MODEL` to `True` to load the base model for text generation + feature probes (heavy); leave `False` for quick SAE-only checks.\n",
    "- Run the cells in order; sparsity + reconstruction metrics help catch bad checkpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5bafa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd/jdh/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports and device pick\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from sae_lens.constants import DTYPE_MAP\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# pick device; default to CPU if CUDA/MPS unavailable\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed3b47b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected run dir: /ssd/jdh/interpretability/experiments/exp-sae-lens/runs/20251213_002417_llama3_8b\n",
      "Translated hook name: model.layers.15.mlp.down_proj -> blocks.15.hook_mlp_out\n",
      "Quantization mode: 4bit\n",
      "Hook: blocks.15.hook_mlp_out, Model: meta-llama/Meta-Llama-3-8B, Context size: 256\n"
     ]
    }
   ],
   "source": [
    "# Locate the most recent run directory (suffix `_llama3_8b`).\n",
    "runs_root = Path(\"runs\")\n",
    "candidate_runs = sorted(\n",
    "    [p for p in runs_root.glob(\"*_llama3_8b\") if p.is_dir()],\n",
    "    key=lambda p: p.stat().st_mtime,\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "# Enter your desired run directory here, or leave as None to auto-select\n",
    "RUN_DIR = None  # e.g., Path(\"runs/20240622-120000_llama3_8b\")\n",
    "if RUN_DIR:\n",
    "    run_dir = Path(RUN_DIR)\n",
    "elif candidate_runs:\n",
    "    run_dir = candidate_runs[0]\n",
    "else:\n",
    "    raise FileNotFoundError(\"No *_llama3_8b run directories found in ./runs/\")\n",
    "print(f\"Selected run dir: {run_dir.resolve()}\")\n",
    "\n",
    "sae_dir = run_dir / \"final_sae\"\n",
    "if not sae_dir.exists():\n",
    "    # fallback to runs/<name> directly if final_sae isn't present\n",
    "    sae_dir = run_dir\n",
    "\n",
    "cfg_path = sae_dir / \"cfg.json\"\n",
    "runner_cfg_path = run_dir / \"runner_cfg.json\"\n",
    "assert cfg_path.exists(), f\"SAE config not found at {cfg_path}\"\n",
    "\n",
    "runner_cfg: dict[str, Any] = json.loads(runner_cfg_path.read_text()) if runner_cfg_path.exists() else {}\n",
    "\n",
    "# Convert dtype strings like \"torch.bfloat16\" to actual dtypes\n",
    "hf_load_kwargs_raw = runner_cfg.get(\"model_from_pretrained_kwargs\", {})\n",
    "\n",
    "def resolve_dtype(val: Any) -> Any:\n",
    "    if isinstance(val, str) and val in DTYPE_MAP:\n",
    "        return DTYPE_MAP[val]\n",
    "    return val\n",
    "\n",
    "hf_load_kwargs = {k: resolve_dtype(v) for k, v in hf_load_kwargs_raw.items()}\n",
    "\n",
    "hook_name = runner_cfg.get(\"hook_name\", \"model.layers.15.mlp.down_proj\")\n",
    "model_name = runner_cfg.get(\"model_name\", \"meta-llama/Meta-Llama-3-8B\")\n",
    "context_size = runner_cfg.get(\"context_size\", 256)\n",
    "\n",
    "# Translate HF module paths to TransformerLens hook names\n",
    "\n",
    "def tl_hook_name_from_hf(path: str) -> str:\n",
    "    m = re.fullmatch(r\"model\\.layers\\.(\\d+)\\.mlp\\.down_proj\", path)\n",
    "    if m:\n",
    "        return f\"blocks.{m.group(1)}.hook_mlp_out\"\n",
    "    m = re.fullmatch(r\"model\\.layers\\.(\\d+)\\.self_attn\\.o_proj\", path)\n",
    "    if m:\n",
    "        return f\"blocks.{m.group(1)}.attn.hook_result\"\n",
    "    return path\n",
    "\n",
    "original_hook_name = hook_name\n",
    "hook_name = tl_hook_name_from_hf(hook_name)\n",
    "if hook_name != original_hook_name:\n",
    "    print(f\"Translated hook name: {original_hook_name} -> {hook_name}\")\n",
    "\n",
    "# Quantization override: \"4bit\" (default), \"none\", or \"from_run\" (use runner cfg as-is)\n",
    "QUANTIZATION_MODE = \"4bit\"\n",
    "allowed_modes = {\"4bit\", \"none\", \"from_run\"}\n",
    "if QUANTIZATION_MODE not in allowed_modes:\n",
    "    raise ValueError(f\"QUANTIZATION_MODE must be one of {allowed_modes}\")\n",
    "\n",
    "if QUANTIZATION_MODE != \"from_run\":\n",
    "    q_kwargs = {k: v for k, v in hf_load_kwargs.items() if not k.startswith(\"load_in\")}\n",
    "    for k in [\"bnb_4bit_quant_type\", \"bnb_4bit_compute_dtype\", \"bnb_4bit_use_double_quant\"]:\n",
    "        q_kwargs.pop(k, None)\n",
    "    if QUANTIZATION_MODE == \"4bit\":\n",
    "        compute_dtype = q_kwargs.get(\"torch_dtype\", torch.bfloat16 if DEVICE != \"cpu\" else torch.float32)\n",
    "        q_kwargs.update({\n",
    "            \"load_in_4bit\": True,\n",
    "            \"bnb_4bit_quant_type\": \"nf4\",\n",
    "            \"bnb_4bit_compute_dtype\": compute_dtype,\n",
    "            \"bnb_4bit_use_double_quant\": True,\n",
    "        })\n",
    "    # QUANTIZATION_MODE == \"none\" leaves q_kwargs without 4/8-bit flags\n",
    "    hf_load_kwargs = q_kwargs\n",
    "print(f\"Quantization mode: {QUANTIZATION_MODE}\")\n",
    "# Note: HookedSAETransformer will receive sanitized kwargs; quantization is driven by the HF model when needed.\n",
    "\n",
    "# Avoid auto offload unless you really want it; meta tensors can break hook usage\n",
    "USE_DEVICE_MAP = False\n",
    "if not USE_DEVICE_MAP:\n",
    "    hf_load_kwargs.pop(\"device_map\", None)\n",
    "    hf_load_kwargs.pop(\"offload_folder\", None)\n",
    "\n",
    "model_load_kwargs = dict(hf_load_kwargs)\n",
    "print(f\"Hook: {hook_name}, Model: {model_name}, Context size: {context_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d171827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SAE arch=standard d_in=4096 d_sae=16384 dtype=torch.float32 hook=blocks.15.hook_mlp_out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd/jdh/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/sae_lens/saes/sae.py:248: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the SAE (always). Model loads happen sequentially to save VRAM.\n",
    "RUN_MODEL = True  # flip to True to run generation/feature probes (heavy)\n",
    "\n",
    "sae = SAE.load_from_disk(sae_dir, device=DEVICE)\n",
    "sae.eval()\n",
    "# Ensure SAE hook matches TransformerLens naming\n",
    "sae.cfg.metadata.hook_name = hook_name\n",
    "print(f\"Loaded SAE arch={sae.cfg.architecture()} d_in={sae.cfg.d_in} d_sae={sae.cfg.d_sae} dtype={sae.dtype} hook={sae.cfg.metadata.hook_name}\")\n",
    "\n",
    "# HookedSAETransformer cannot accept raw bitsandbytes flags via from_pretrained_kwargs.\n",
    "# Build a quantized HF model when needed and strip quant flags for TransformerLens kwargs.\n",
    "_intervention_blocklist = {\n",
    "    \"load_in_4bit\",\n",
    "    \"load_in_8bit\",\n",
    "    \"bnb_4bit_quant_type\",\n",
    "    \"bnb_4bit_compute_dtype\",\n",
    "    \"bnb_4bit_use_double_quant\",\n",
    "    \"from_pretrained_kwargs\",\n",
    "}\n",
    "intervention_load_kwargs = {k: v for k, v in model_load_kwargs.items() if k not in _intervention_blocklist}\n",
    "\n",
    "\n",
    "def wants_4bit_quant() -> bool:\n",
    "    return QUANTIZATION_MODE == \"4bit\" or (\n",
    "        QUANTIZATION_MODE == \"from_run\" and hf_load_kwargs.get(\"load_in_4bit\")\n",
    "    )\n",
    "\n",
    "\n",
    "def build_quantized_hf_model():\n",
    "    if DEVICE == \"cpu\":\n",
    "        raise ValueError(\"4-bit quantization requires CUDA; switch QUANTIZATION_MODE or device.\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=model_load_kwargs.get(\"bnb_4bit_quant_type\", \"nf4\"),\n",
    "        bnb_4bit_compute_dtype=model_load_kwargs.get(\n",
    "            \"bnb_4bit_compute_dtype\",\n",
    "            model_load_kwargs.get(\"torch_dtype\", torch.bfloat16),\n",
    "        ),\n",
    "        bnb_4bit_use_double_quant=model_load_kwargs.get(\"bnb_4bit_use_double_quant\", True),\n",
    "    )\n",
    "    hf_kwargs = {\n",
    "        k: v\n",
    "        for k, v in model_load_kwargs.items()\n",
    "        if not k.startswith(\"load_in\") and not k.startswith(\"bnb_4bit\")\n",
    "    }\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\" if USE_DEVICE_MAP else None,\n",
    "        **hf_kwargs,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    return hf_model, tokenizer\n",
    "\n",
    "\n",
    "def load_base_model():\n",
    "    if wants_4bit_quant():\n",
    "        hf_model, tokenizer = build_quantized_hf_model()\n",
    "        return HookedSAETransformer.from_pretrained(\n",
    "            model_name,\n",
    "            hf_model=hf_model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=DEVICE,\n",
    "            move_to_device=False,  # HF already placed weights according to device_map\n",
    "            from_pretrained_kwargs=intervention_load_kwargs,\n",
    "        )\n",
    "\n",
    "    return HookedSAETransformer.from_pretrained(\n",
    "        model_name,\n",
    "        device=DEVICE,\n",
    "        from_pretrained_kwargs=intervention_load_kwargs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefa7dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated active features per token (L0): 3135.3 / 16384\n"
     ]
    }
   ],
   "source": [
    "# Quick sparsity snapshot\n",
    "sparsity_path = run_dir / \"sparsity.safetensors\"\n",
    "if not sparsity_path.exists():\n",
    "    alt = sae_dir / \"sparsity.safetensors\"\n",
    "    sparsity_path = alt if alt.exists() else None\n",
    "\n",
    "if sparsity_path and sparsity_path.exists():\n",
    "    log_sparsity = load_file(sparsity_path)[\"sparsity\"]\n",
    "    est_l0 = torch.exp(log_sparsity).mean().item() * log_sparsity.numel()\n",
    "    print(f\"Estimated active features per token (L0): {est_l0:.1f} / {log_sparsity.numel()}\")\n",
    "else:\n",
    "    print(\"No sparsity file found; skipping sparsity estimate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "663f9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper utilities for sequential generation/loss and feature grabs.\n",
    "import contextlib\n",
    "\n",
    "prompt_text = \"Once upon a time, a curious robot learned to help humans\"\n",
    "\n",
    "def run_generation_and_loss(with_sae: bool = False):\n",
    "    model = load_base_model()\n",
    "    ctx = model.saes(saes=[sae]) if with_sae else contextlib.nullcontext()\n",
    "    with torch.no_grad(), ctx:\n",
    "        story = model.generate(\n",
    "            prompt_text,\n",
    "            max_new_tokens=80,\n",
    "            temperature=0.7,\n",
    "            stop_at_eos=True,\n",
    "            verbose=False,\n",
    "        )\n",
    "        tokens = model.to_tokens(prompt_text, prepend_bos=True)\n",
    "        logits = model(tokens)\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            logits[0, :-1].reshape(-1, logits.size(-1)),\n",
    "            tokens[0, 1:].reshape(-1),\n",
    "        ).item()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    return story, loss\n",
    "\n",
    "def grab_feature_activations():\n",
    "    model = load_base_model()\n",
    "    with torch.no_grad():\n",
    "        tokens = model.to_tokens(prompt_text, prepend_bos=True)\n",
    "        _, cache = model.run_with_cache(tokens, names_filter=[hook_name])\n",
    "        mlp_out = cache[hook_name]\n",
    "        feature_acts = sae.encode(mlp_out)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    return mlp_out, feature_acts\n",
    "\n",
    "def feature_subset_logits(num_features: int = 10):\n",
    "    model = load_base_model()\n",
    "    with torch.no_grad():\n",
    "        tokens = model.to_tokens(prompt_text, prepend_bos=True)\n",
    "        base_logits = model(tokens)\n",
    "        _, cache = model.run_with_cache(tokens, names_filter=[hook_name])\n",
    "        mlp_out = cache[hook_name]\n",
    "        feature_acts = sae.encode(mlp_out)\n",
    "\n",
    "        rand_idx = torch.randperm(feature_acts.size(-1), device=feature_acts.device)[:num_features]\n",
    "        kept_features = torch.zeros_like(feature_acts)\n",
    "        kept_features[..., rand_idx] = feature_acts[..., rand_idx]\n",
    "        recon_from_subset = sae.decode(kept_features)\n",
    "\n",
    "        def swap_mlp_out(acts, hook):\n",
    "            return recon_from_subset\n",
    "\n",
    "        hooked_logits = model.run_with_hooks(\n",
    "            tokens, fwd_hooks=[(hook_name, swap_mlp_out)]\n",
    "        )\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    return base_logits, hooked_logits, rand_idx\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "692cd3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.76s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8388608) must match the size of tensor b (4096) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compare generations and prompt loss with/without SAE reconstruction (sequential loads).\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RUN_MODEL:\n\u001b[0;32m----> 3\u001b[0m     base_story, base_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_generation_and_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwith_sae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     sae_story, sae_loss \u001b[38;5;241m=\u001b[39m run_generation_and_loss(with_sae\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Base model ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mrun_generation_and_loss\u001b[0;34m(with_sae)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_generation_and_loss\u001b[39m(with_sae: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 7\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msaes(saes\u001b[38;5;241m=\u001b[39m[sae]) \u001b[38;5;28;01mif\u001b[39;00m with_sae \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), ctx:\n",
      "Cell \u001b[0;32mIn[3], line 59\u001b[0m, in \u001b[0;36mload_base_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wants_4bit_quant():\n\u001b[1;32m     58\u001b[0m     hf_model, tokenizer \u001b[38;5;241m=\u001b[39m build_quantized_hf_model()\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHookedSAETransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmove_to_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# HF already placed weights according to device_map\u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_pretrained_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintervention_load_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HookedSAETransformer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     69\u001b[0m     model_name,\n\u001b[1;32m     70\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE,\n\u001b[1;32m     71\u001b[0m     from_pretrained_kwargs\u001b[38;5;241m=\u001b[39mintervention_load_kwargs,\n\u001b[1;32m     72\u001b[0m )\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1382\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, first_n_layers, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# Create the HookedTransformer object\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m   1376\u001b[0m     cfg,\n\u001b[1;32m   1377\u001b[0m     tokenizer,\n\u001b[1;32m   1378\u001b[0m     move_to_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1379\u001b[0m     default_padding_side\u001b[38;5;241m=\u001b[39mdefault_padding_side,\n\u001b[1;32m   1380\u001b[0m )\n\u001b[0;32m-> 1382\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_process_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_ln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter_unembed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter_unembed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_value_biases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_value_biases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m move_to_device:\n\u001b[1;32m   1392\u001b[0m     model\u001b[38;5;241m.\u001b[39mmove_model_modules_to_device()\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1615\u001b[0m, in \u001b[0;36mHookedTransformer.load_and_process_state_dict\u001b[0;34m(self, state_dict, fold_ln, center_writing_weights, center_unembed, fold_value_biases, refactor_factored_attn_matrices)\u001b[0m\n\u001b[1;32m   1613\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfold_layer_norm(state_dict)\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mnormalization_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSPre\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 1615\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold_layer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_biases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1619\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are not using LayerNorm or RMSNorm, so the layer norm weights can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be folded! Skipping\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1621\u001b[0m     )\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1705\u001b[0m, in \u001b[0;36mHookedTransformer.fold_layer_norm\u001b[0;34m(self, state_dict, fold_biases, center_weights)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgqa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mb_V\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m state_dict[\n\u001b[1;32m   1695\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgqa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mb_V\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1696\u001b[0m     ] \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1700\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1701\u001b[0m     )\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ln1.b\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1704\u001b[0m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.W_Q\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1705\u001b[0m     \u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ml\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.attn.W_Q\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ml\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.ln1.w\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[1;32m   1706\u001b[0m )\n\u001b[1;32m   1707\u001b[0m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgqa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mW_K\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1708\u001b[0m     state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgqa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mW_K\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1709\u001b[0m     \u001b[38;5;241m*\u001b[39m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ln1.w\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m   1710\u001b[0m )\n\u001b[1;32m   1711\u001b[0m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgqa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mW_V\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1712\u001b[0m     state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgqa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mW_V\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1713\u001b[0m     \u001b[38;5;241m*\u001b[39m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ln1.w\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m   1714\u001b[0m )\n",
      "File \u001b[0;32m~/interpretability/experiments/exp-sae-lens/.venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:393\u001b[0m, in \u001b[0;36mParams4bit.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    382\u001b[0m             data\u001b[38;5;241m=\u001b[39mresult,\n\u001b[1;32m    383\u001b[0m             requires_grad\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mrequires_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m             bnb_quantized\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mbnb_quantized,\n\u001b[1;32m    391\u001b[0m         )\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8388608) must match the size of tensor b (4096) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Compare generations and prompt loss with/without SAE reconstruction (sequential loads).\n",
    "if RUN_MODEL:\n",
    "    base_story, base_loss = run_generation_and_loss(with_sae=False)\n",
    "    sae_story, sae_loss = run_generation_and_loss(with_sae=True)\n",
    "\n",
    "    print(\"--- Base model ---\")\n",
    "    print(base_story)\n",
    "    print(\"--- With SAE reconstruction ---\")\n",
    "    print(sae_story)\n",
    "    print(f\"Next-token loss without SAE: {base_loss:.3f}\")\n",
    "    print(f\"Next-token loss with SAE:    {sae_loss:.3f}\")\n",
    "else:\n",
    "    print(\"Set RUN_MODEL=True to run generation and loss comparisons.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c83dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top activated SAE features on the last token of the prompt (sequential load).\n",
    "if RUN_MODEL:\n",
    "    _, feature_acts = grab_feature_activations()\n",
    "    last_token_acts = feature_acts[0, -1]\n",
    "    values, indices = torch.topk(last_token_acts, k=10)\n",
    "    print(\"Top activated features on the last token:\")\n",
    "    for val, idx in zip(values.tolist(), indices.tolist()):\n",
    "        print(f\"  Feature {idx:5d} -> activation {val:.2f}\")\n",
    "else:\n",
    "    print(\"Set RUN_MODEL=True to inspect activated features.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of a random subset of SAE features on logits for the prompt (sequential load).\n",
    "if RUN_MODEL:\n",
    "    base_logits, hooked_logits, rand_idx = feature_subset_logits(num_features=10)\n",
    "\n",
    "    def show_top_ids(logits, label, k=10):\n",
    "        probs = logits.softmax(-1)\n",
    "        vals, idxs = torch.topk(probs, k)\n",
    "        print(label)\n",
    "        for v, i in zip(vals.tolist(), idxs.tolist()):\n",
    "            print(f\"  token_id={i:5d} prob={v:.3f}\")\n",
    "        print()\n",
    "\n",
    "    base_last = base_logits[0, -1]\n",
    "    hooked_last = hooked_logits[0, -1]\n",
    "    print(f\"Random feature IDs: {sorted(rand_idx.tolist())}\")\n",
    "    show_top_ids(base_last, \"Base model last-token distribution:\")\n",
    "    show_top_ids(hooked_last, \"Only these features reconstructed:\")\n",
    "else:\n",
    "    print(\"Set RUN_MODEL=True to run feature-subset logits.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2edaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction quality on the prompt (sequential). Falls back to synthetic if RUN_MODEL=False.\n",
    "if RUN_MODEL:\n",
    "    mlp_out, feature_acts = grab_feature_activations()\n",
    "else:\n",
    "    mlp_out = torch.randn(4, 8, sae.cfg.d_in, device=sae.device, dtype=sae.dtype)\n",
    "    feature_acts = sae.encode(mlp_out)\n",
    "\n",
    "with torch.no_grad():\n",
    "    recon = sae.decode(feature_acts)\n",
    "\n",
    "    mse = torch.nn.functional.mse_loss(recon, mlp_out).item()\n",
    "    l2_orig = mlp_out.pow(2).mean().sqrt().item()\n",
    "    l2_err = (recon - mlp_out).pow(2).mean().sqrt().item()\n",
    "    explained = 1 - (l2_err**2 / (l2_orig**2 + 1e-9))\n",
    "    density = (feature_acts.abs() > 1e-6).float().mean().item()\n",
    "\n",
    "print(\n",
    "    f\"Recon MSE: {mse:.6f}\n",
    "\"\n",
    "    f\"Orig L2 (mean): {l2_orig:.4f}\n",
    "\"\n",
    "    f\"Error L2 (mean): {l2_err:.4f}\n",
    "\"\n",
    "    f\"Explained variance (approx): {explained:.4f}\n",
    "\"\n",
    "    f\"Mean activation density: {density:.4f}\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2351ff",
   "metadata": {},
   "source": [
    "### Reading the diagnostics\n",
    "- High `Recon MSE` / `Error L2` or low `Explained variance` may indicate a bad checkpoint or mismatched hook.\n",
    "- `Mean activation density` tracks sparsity; big jumps suggest L1/normalization changes.\n",
    "- Compare base vs SAE next-token loss to see whether reconstruction is neutral or harmful for the prompt.\n",
    "- Top-activated features and feature-subset logits help sanity-check which directions the SAE is using.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp-sae-lens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
