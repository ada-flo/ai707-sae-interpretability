# Interpretability with Sparse Autoencoders (SAEs)

This project focuses on **interpretability** research for Large Language Models using **Sparse Autoencoders (SAEs)**.

It serves as a workspace for exploring, analyzing, and training SAEs to understand internal model activations. The codebase largely follows the "batteries-included" approach recommended in the documentation, utilizing tools such as:

- **SAELens**: For loading, analyzing, and training SAEs.
- **Gemma Scope**: For exploring pre-trained SAEs on Gemma models.
- **TransformerLens**: For hooking into models and capturing activations.

## Structure

- **`docs/`**: Contains guidelines and onboarding resources (e.g., path to mastering SAEs).
- **`experiments/`**: Stores experimental code and notebooks (e.g., `exp-sae-lens`).

## WanDB Acc

lasip21784@alexida.com
Aa00990099